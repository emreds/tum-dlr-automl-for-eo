{"cells":[{"cell_type":"markdown","metadata":{"collapsed":false,"id":"67C064920AA545848F2A3FD934CE3639","jupyter":{},"notebookId":"635ff9c1addc7648de76ef79","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["# Environment and setup"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"5B242777C30444038CBE547B302FBDB3","jupyter":{},"notebookId":"635ff9c1addc7648de76ef79","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["The NASLib library is recommended for use on Linux machines.\n","While installing the repository, creating a new conda environment is recomended. Install PyTorch GPU/CPU for your setup.\n","```\n","conda create -n mvenv python=3.7\n","conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c nvidia\n","```"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"6B4FAF60016F41DF9E0B4B0D1B158F22","jupyter":{},"notebookId":"635ff9c1addc7648de76ef79","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["Run setup.py file with the following command, which will install all the packages listed in [requirements.txt](https://github.com/automl/NASLib/blob/Develop/requirements.txt)\n","\n","```\n","pip install --upgrade pip setuptools wheel\n","pip install -e .\n","```\n","\n","Or you can just clone this repository\n","\n","```\n","git clone https://github.com/emreds/NASLib.git\n","cd NASLib\n","```\n","\n","Afterwards, install NASLib by running the setup.py file in developer mode:\n","\n","```\n","python3 setup.py develop\n","\n","```"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"906EEC3FA630428DA66C51A57AE3E4A6","jupyter":{},"notebookId":"635ff9c1addc7648de76ef79","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["In a high-level NASLib consists of 4 main building blocks which (can) interact with each other:\n","* search spaces (cell search space, hierarchical, ...)\n","* optimizers (one-shot/weight-sharing optimizers, black-box optimizers)\n","* predictors (performance estimators that given an architecture as input, output its performance)\n","* evaluators (run the architecture search loop and the final network training pipeline)\n","\n","![Image Name](https://cdn.kesci.com/upload/image/rkt4qna0p8.png?imageView2/0/w/960/h/960)\n","\n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"12A129E06E41479A978D2473BF973308","jupyter":{},"notebookId":"635ff9c1addc7648de76ef79","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["# Caseï¼š NAS-Bench-101"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"id":"B83E496D45B648F1875FAD0DBAFD93C7","jupyter":{},"notebookId":"635ff9c1addc7648de76ef79","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["device: cuda:0\n","device: cpu\n","device: cuda:0\n","device: cuda:0\n","device: cuda:0\n","device: cuda:0\n"]}],"source":["#Import packages\n","import logging\n","import sys\n","\n","from naslib.defaults.trainer import Trainer\n","from naslib.optimizers import (\n","    DARTSOptimizer,\n","    GDASOptimizer,\n","    DrNASOptimizer,\n","    RandomSearch,\n","    RegularizedEvolution,\n","    LocalSearch,\n","    Bananas,\n","    BasePredictor,\n",")\n","\n","from naslib.search_spaces import (\n","    DartsSearchSpace,\n","    SimpleCellSearchSpace,\n","    NasBench101SearchSpace,\n","    HierarchicalSearchSpace,\n",")\n","from naslib.search_spaces.nasbench101 import graph\n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"3E153E78842347B089ED7AE2095391F3","jupyter":{},"notebookId":"635ff9c1addc7648de76ef79","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["## Running the search"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"id":"553693AE25324A03848E4F9639C69CE3","jupyter":{},"notebookId":"635ff9c1addc7648de76ef79","scrolled":true,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[32m[11/04 04:49:17 nl.utils.utils]: \u001b[0mdataset....................................cifar10\n","\u001b[32m[11/04 04:49:17 nl.utils.utils]: \u001b[0mseed............................................99\n","\u001b[32m[11/04 04:49:17 nl.utils.utils]: \u001b[0msearch_space...........................nasbench101\n","\u001b[32m[11/04 04:49:17 nl.utils.utils]: \u001b[0mout_dir........................................run\n","\u001b[32m[11/04 04:49:17 nl.utils.utils]: \u001b[0moptimizer....................................darts\n","\u001b[32m[11/04 04:49:17 nl.utils.utils]: \u001b[0msearchacq_fn_optimization: mutation\n","acq_fn_type: its\n","arch_learning_rate: 0.0003\n","arch_weight_decay: 0.001\n","batch_size: 64\n","checkpoint_freq: 5\n","cutout: False\n","cutout_length: 16\n","cutout_prob: 1.0\n","data_size: 25000\n","debug_predictor: False\n","drop_path_prob: 0.0\n","encoding_type: path\n","epochs: 10\n","fidelity: 200\n","gpu: None\n","grad_clip: 5\n","k: 10\n","learning_rate: 0.025\n","learning_rate_min: 0.001\n","max_mutations: 1\n","momentum: 0.9\n","num_arches_to_mutate: 2\n","num_candidates: 100\n","num_ensemble: 3\n","num_init: 10\n","output_weights: True\n","population_size: 100\n","predictor_type: var_sparse_gp\n","sample_size: 10\n","seed: 99\n","tau_max: 10\n","tau_min: 0.1\n","train_portion: 0.5\n","unrolled: False\n","warm_start_epochs: 0\n","weight_decay: 0.0003\n","\u001b[32m[11/04 04:49:17 nl.utils.utils]: \u001b[0mevaluationauxiliary_weight: 0.4\n","batch_size: 96\n","checkpoint_freq: 30\n","cutout: True\n","cutout_length: 16\n","cutout_prob: 1.0\n","data_size: 50000\n","dist_backend: nccl\n","dist_url: tcp://127.0.0.1:8888\n","drop_path_prob: 0.2\n","epochs: 600\n","gpu: None\n","grad_clip: 5\n","learning_rate: 0.025\n","learning_rate_min: 0.0\n","momentum: 0.9\n","multiprocessing_distributed: False\n","rank: 0\n","train_portion: 1.0\n","warm_start_epochs: 0\n","weight_decay: 0.0003\n","world_size: 1\n","\u001b[32m[11/04 04:49:17 nl.utils.utils]: \u001b[0meval_only....................................False\n","\u001b[32m[11/04 04:49:17 nl.utils.utils]: \u001b[0mresume.......................................False\n","\u001b[32m[11/04 04:49:17 nl.utils.utils]: \u001b[0mmodel_path....................................None\n","\u001b[32m[11/04 04:49:17 nl.utils.utils]: \u001b[0mgpu...........................................None\n","\u001b[32m[11/04 04:49:17 nl.utils.utils]: \u001b[0msave..............run/nasbench101/cifar10/darts/99\n","\u001b[32m[11/04 04:49:17 nl.utils.utils]: \u001b[0mdata................../home/mw/project/naslib/data\n"]}],"source":["# import some utilities and parse the configuration file\n","\n","from naslib.utils import utils, setup_logger\n","\n","# Read args and config, setup logger\n","config = utils.get_config_from_args()\n","utils.set_seed(config.seed)\n","\n","logger = setup_logger(config.save + \"/log.log\")\n","# logger.setLevel(logging.INFO)   \n","\n","# This will read the parameters from the default yaml configuration file, which in this \n","# case is located in NASLib/naslib/benchmarks/nas_predictors/discrete_config.yaml.\n","# You can change the parameters.\n","#\n","\n","\n","utils.log_args(config)\n","\n","supported_optimizers = {\n","    \"darts\": DARTSOptimizer(config),\n","    \"gdas\": GDASOptimizer(config),\n","    \"drnas\": DrNASOptimizer(config),\n","    \"rs\": RandomSearch(config),\n","    \"re\": RegularizedEvolution(config),\n","    \"ls\": LocalSearch(config),\n","    \"bananas\": Bananas(config),\n","    \"bp\": BasePredictor(config),\n","}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"id":"6493329A2DA140BB9E5A01D9A59987FD","jupyter":{},"notebookId":"635ff9c1addc7648de76ef79","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/04 04:49:30 nl.search_spaces.core.graph]: \u001b[0mUpdate function could not be verified. Be cautious with the setting of `private_edge_data` in `update_edges()`\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/04 04:49:30 nl.search_spaces.core.graph]: \u001b[0mUpdate function could not be verified. Be cautious with the setting of `private_edge_data` in `update_edges()`\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/04 04:49:30 nl.search_spaces.core.graph]: \u001b[0mUpdate function could not be verified. Be cautious with the setting of `private_edge_data` in `update_edges()`\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/04 04:49:30 nl.search_spaces.core.graph]: \u001b[0mUpdate function could not be verified. Be cautious with the setting of `private_edge_data` in `update_edges()`\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/04 04:49:30 nl.search_spaces.core.graph]: \u001b[0mUpdate function could not be verified. Be cautious with the setting of `private_edge_data` in `update_edges()`\n"]}],"source":["search_space = graph.NasBench101SearchSpace()"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"id":"84FFABD7E95B45C1859424A97FBACB5B","jupyter":{},"notebookId":"635ff9c1addc7648de76ef79","scrolled":true,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[32m[11/04 04:49:54 nl.defaults.trainer]: \u001b[0mparam size = 1.405086MB\n","\u001b[32m[11/04 04:49:54 nl.defaults.trainer]: \u001b[0mStart training\n","Files already downloaded and verified\n","Files already downloaded and verified\n","\u001b[32m[11/04 04:49:56 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n","+0.000795, +0.001128, 1\n","+0.000025, +0.000409, 1\n","+0.000276, -0.000359, 0\n","-0.002889, -0.000648, 1\n","+0.000897, +0.000638, 0\n","-0.000261, +0.001346, 1\n","-0.000104, -0.001615, 0\n","-0.000818, +0.000548, 1\n","+0.001200, +0.001071, 0\n","+0.000581, -0.000051, 0\n","-0.000496, -0.000710, 0\n","-0.000978, -0.000795, 1\n","+0.000197, +0.001445, 1\n","-0.001014, +0.000702, 1\n","-0.002228, -0.000983, 1\n","-0.000764, +0.000155, 1\n","-0.000734, -0.000382, 1\n","+0.000194, +0.001900, 1\n","-0.000400, +0.000318, 1\n","+0.000545, -0.000949, 0\n","+0.000318, -0.001719, 0\n","-0.000660, +0.000578, -0.001648, 1\n","-0.001182, +0.000130, -0.001794, 1\n","-0.000984, +0.000257, -0.000422, 1\n","-0.001851, +0.000126, -0.000635, 1\n","+0.000451, -0.000942, -0.000960, 0\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/04 04:49:56 nl.search_spaces.core.graph]: \u001b[0mComb_op is ignored if subgraph is defined!\n","\u001b[32m[11/04 04:49:57 nl.defaults.trainer]: \u001b[0mEpoch 0-0, Train loss: 2.31591, validation loss: 2.63751, learning rate: [0.025]\n","\u001b[32m[11/04 04:49:57 nl.defaults.trainer]: \u001b[0mcuda consumption\n"," |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |   18875 KB |    1440 MB |   18719 MB |   18701 MB |\n","|       from large pool |       0 KB |    1267 MB |   16326 MB |   16326 MB |\n","|       from small pool |   18875 KB |     195 MB |    2393 MB |    2375 MB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |   18875 KB |    1440 MB |   18719 MB |   18701 MB |\n","|       from large pool |       0 KB |    1267 MB |   16326 MB |   16326 MB |\n","|       from small pool |   18875 KB |     195 MB |    2393 MB |    2375 MB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |    1492 MB |    1492 MB |    1492 MB |       0 B  |\n","|       from large pool |    1288 MB |    1288 MB |    1288 MB |       0 B  |\n","|       from small pool |     204 MB |     204 MB |     204 MB |       0 B  |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |  136772 KB |  243474 KB |   15098 MB |   14964 MB |\n","|       from large pool |       0 KB |  111104 KB |   12548 MB |   12548 MB |\n","|       from small pool |  136772 KB |  142418 KB |    2549 MB |    2416 MB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |    2507    |    3148    |   20277    |   17770    |\n","|       from large pool |       0    |     298    |    3620    |    3620    |\n","|       from small pool |    2507    |    2853    |   16657    |   14150    |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |    2507    |    3148    |   20277    |   17770    |\n","|       from large pool |       0    |     298    |    3620    |    3620    |\n","|       from small pool |    2507    |    2853    |   16657    |   14150    |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |     173    |     173    |     173    |       0    |\n","|       from large pool |      71    |      71    |      71    |       0    |\n","|       from small pool |     102    |     102    |     102    |       0    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |     128    |     463    |   13312    |   13184    |\n","|       from large pool |       0    |      26    |    2866    |    2866    |\n","|       from small pool |     128    |     463    |   10446    |   10318    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","\u001b[32m[11/04 04:49:58 nl.defaults.trainer]: \u001b[0mcuda consumption\n"," |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |   18875 KB |    1445 MB |   37435 MB |   37417 MB |\n","|       from large pool |       0 KB |    1267 MB |   32659 MB |   32659 MB |\n","|       from small pool |   18875 KB |     200 MB |    4776 MB |    4757 MB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |   18875 KB |    1445 MB |   37435 MB |   37417 MB |\n","|       from large pool |       0 KB |    1267 MB |   32659 MB |   32659 MB |\n","|       from small pool |   18875 KB |     200 MB |    4776 MB |    4757 MB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |    1498 MB |    1498 MB |    1498 MB |       0 B  |\n","|       from large pool |    1288 MB |    1288 MB |    1288 MB |       0 B  |\n","|       from small pool |     210 MB |     210 MB |     210 MB |       0 B  |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |  167492 KB |  268529 KB |   30508 MB |   30345 MB |\n","|       from large pool |       0 KB |  111104 KB |   25513 MB |   25513 MB |\n","|       from small pool |  167492 KB |  167492 KB |    4994 MB |    4831 MB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |    2507    |    3754    |   38674    |   36167    |\n","|       from large pool |       0    |     298    |    7240    |    7240    |\n","|       from small pool |    2507    |    3459    |   31434    |   28927    |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |    2507    |    3754    |   38674    |   36167    |\n","|       from large pool |       0    |     298    |    7240    |    7240    |\n","|       from small pool |    2507    |    3459    |   31434    |   28927    |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |     176    |     176    |     176    |       0    |\n","|       from large pool |      71    |      71    |      71    |       0    |\n","|       from small pool |     105    |     105    |     105    |       0    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |     375    |     483    |   26483    |   26108    |\n","|       from large pool |       0    |      26    |    5742    |    5742    |\n","|       from small pool |     375    |     483    |   20741    |   20366    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","\u001b[32m[11/04 04:49:58 nl.defaults.trainer]: \u001b[0mcuda consumption\n"," |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |   18875 KB |    1445 MB |   56151 MB |   56133 MB |\n","|       from large pool |       0 KB |    1267 MB |   48993 MB |   48993 MB |\n","|       from small pool |   18875 KB |     200 MB |    7158 MB |    7140 MB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |   18875 KB |    1445 MB |   56151 MB |   56133 MB |\n","|       from large pool |       0 KB |    1267 MB |   48993 MB |   48993 MB |\n","|       from small pool |   18875 KB |     200 MB |    7158 MB |    7140 MB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |    1498 MB |    1498 MB |    1498 MB |       0 B  |\n","|       from large pool |    1288 MB |    1288 MB |    1288 MB |       0 B  |\n","|       from small pool |     210 MB |     210 MB |     210 MB |       0 B  |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |  165444 KB |  272628 KB |   45899 MB |   45738 MB |\n","|       from large pool |       0 KB |  111104 KB |   38479 MB |   38479 MB |\n","|       from small pool |  165444 KB |  171572 KB |    7419 MB |    7258 MB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |    2507    |    3754    |   57071    |   54564    |\n","|       from large pool |       0    |     298    |   10860    |   10860    |\n","|       from small pool |    2507    |    3459    |   46211    |   43704    |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |    2507    |    3754    |   57071    |   54564    |\n","|       from large pool |       0    |     298    |   10860    |   10860    |\n","|       from small pool |    2507    |    3459    |   46211    |   43704    |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |     176    |     176    |     176    |       0    |\n","|       from large pool |      71    |      71    |      71    |       0    |\n","|       from small pool |     105    |     105    |     105    |       0    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |     380    |     483    |   39663    |   39283    |\n","|       from large pool |       0    |      26    |    8618    |    8618    |\n","|       from small pool |     380    |     483    |   31045    |   30665    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","\u001b[32m[11/04 04:50:03 nl.defaults.trainer]: \u001b[0mEpoch 0-7, Train loss: 2.45511, validation loss: 2.46617, learning rate: [0.025]\n","\u001b[32m[11/04 04:50:08 nl.defaults.trainer]: \u001b[0mEpoch 0-14, Train loss: 2.76310, validation loss: 2.43974, learning rate: [0.025]\n","\u001b[32m[11/04 04:50:13 nl.defaults.trainer]: \u001b[0mEpoch 0-20, Train loss: 2.46706, validation loss: 2.35705, learning rate: [0.025]\n","\u001b[32m[11/04 04:50:19 nl.defaults.trainer]: \u001b[0mEpoch 0-27, Train loss: 2.54473, validation loss: 2.33002, learning rate: [0.025]\n","\u001b[32m[11/04 04:50:25 nl.defaults.trainer]: \u001b[0mEpoch 0-34, Train loss: 2.45916, validation loss: 2.37604, learning rate: [0.025]\n","\u001b[32m[11/04 04:50:30 nl.defaults.trainer]: \u001b[0mEpoch 0-40, Train loss: 3.19902, validation loss: 2.46966, learning rate: [0.025]\n","\u001b[32m[11/04 04:50:36 nl.defaults.trainer]: \u001b[0mEpoch 0-47, Train loss: 2.65936, validation loss: 2.50070, learning rate: [0.025]\n","\u001b[32m[11/04 04:50:42 nl.defaults.trainer]: \u001b[0mEpoch 0-54, Train loss: 2.83620, validation loss: 2.27281, learning rate: [0.025]\n","\u001b[32m[11/04 04:50:48 nl.defaults.trainer]: \u001b[0mEpoch 0-61, Train loss: 2.46580, validation loss: 2.28546, learning rate: [0.025]\n","\u001b[32m[11/04 04:50:54 nl.defaults.trainer]: \u001b[0mEpoch 0-68, Train loss: 2.72980, validation loss: 2.59485, learning rate: [0.025]\n","\u001b[32m[11/04 04:50:59 nl.defaults.trainer]: \u001b[0mEpoch 0-74, Train loss: 2.57629, validation loss: 2.27522, learning rate: [0.025]\n","\u001b[32m[11/04 04:51:05 nl.defaults.trainer]: \u001b[0mEpoch 0-81, Train loss: 2.27950, validation loss: 2.88241, learning rate: [0.025]\n","\u001b[32m[11/04 04:51:10 nl.defaults.trainer]: \u001b[0mEpoch 0-88, Train loss: 2.38357, validation loss: 2.68175, learning rate: [0.025]\n","\u001b[32m[11/04 04:51:16 nl.defaults.trainer]: \u001b[0mEpoch 0-94, Train loss: 2.23309, validation loss: 2.22403, learning rate: [0.025]\n","\u001b[32m[11/04 04:51:21 nl.defaults.trainer]: \u001b[0mEpoch 0-101, Train loss: 2.34059, validation loss: 2.56612, learning rate: [0.025]\n","\u001b[32m[11/04 04:51:27 nl.defaults.trainer]: \u001b[0mEpoch 0-108, Train loss: 2.26119, validation loss: 2.30341, learning rate: [0.025]\n","\u001b[32m[11/04 04:51:32 nl.defaults.trainer]: \u001b[0mEpoch 0-114, Train loss: 3.16043, validation loss: 2.84115, learning rate: [0.025]\n","\u001b[32m[11/04 04:51:38 nl.defaults.trainer]: \u001b[0mEpoch 0-121, Train loss: 2.45692, validation loss: 2.24828, learning rate: [0.025]\n","\u001b[32m[11/04 04:51:43 nl.defaults.trainer]: \u001b[0mEpoch 0-127, Train loss: 2.61812, validation loss: 2.43163, learning rate: [0.025]\n","\u001b[32m[11/04 04:51:48 nl.defaults.trainer]: \u001b[0mEpoch 0-133, Train loss: 2.43114, validation loss: 2.17061, learning rate: [0.025]\n","\u001b[32m[11/04 04:51:53 nl.defaults.trainer]: \u001b[0mEpoch 0-139, Train loss: 2.68117, validation loss: 2.39612, learning rate: [0.025]\n","\u001b[32m[11/04 04:51:58 nl.defaults.trainer]: \u001b[0mEpoch 0-145, Train loss: 2.51190, validation loss: 2.29155, learning rate: [0.025]\n","\u001b[32m[11/04 04:52:04 nl.defaults.trainer]: \u001b[0mEpoch 0-151, Train loss: 2.21714, validation loss: 2.76691, learning rate: [0.025]\n","\u001b[32m[11/04 04:52:09 nl.defaults.trainer]: \u001b[0mEpoch 0-158, Train loss: 2.29204, validation loss: 2.69979, learning rate: [0.025]\n","\u001b[32m[11/04 04:52:14 nl.defaults.trainer]: \u001b[0mEpoch 0-164, Train loss: 2.18570, validation loss: 2.41026, learning rate: [0.025]\n","\u001b[32m[11/04 04:52:20 nl.defaults.trainer]: \u001b[0mEpoch 0-171, Train loss: 2.39729, validation loss: 2.26257, learning rate: [0.025]\n","\u001b[32m[11/04 04:52:26 nl.defaults.trainer]: \u001b[0mEpoch 0-178, Train loss: 2.16836, validation loss: 2.34997, learning rate: [0.025]\n","\u001b[32m[11/04 04:52:31 nl.defaults.trainer]: \u001b[0mEpoch 0-184, Train loss: 2.27701, validation loss: 1.88103, learning rate: [0.025]\n","\u001b[32m[11/04 04:52:36 nl.defaults.trainer]: \u001b[0mEpoch 0-190, Train loss: 2.66582, validation loss: 2.55595, learning rate: [0.025]\n","\u001b[32m[11/04 04:52:42 nl.defaults.trainer]: \u001b[0mEpoch 0-197, Train loss: 2.46881, validation loss: 2.73198, learning rate: [0.025]\n","\u001b[32m[11/04 04:52:47 nl.defaults.trainer]: \u001b[0mEpoch 0-203, Train loss: 2.32474, validation loss: 2.46025, learning rate: [0.025]\n","\u001b[32m[11/04 04:52:53 nl.defaults.trainer]: \u001b[0mEpoch 0-210, Train loss: 2.33071, validation loss: 2.18532, learning rate: [0.025]\n","\u001b[32m[11/04 04:52:58 nl.defaults.trainer]: \u001b[0mEpoch 0-216, Train loss: 2.51230, validation loss: 2.65254, learning rate: [0.025]\n","\u001b[32m[11/04 04:53:03 nl.defaults.trainer]: \u001b[0mEpoch 0-222, Train loss: 2.45531, validation loss: 2.09358, learning rate: [0.025]\n","\u001b[32m[11/04 04:53:08 nl.defaults.trainer]: \u001b[0mEpoch 0-228, Train loss: 2.52088, validation loss: 2.69304, learning rate: [0.025]\n","\u001b[32m[11/04 04:53:13 nl.defaults.trainer]: \u001b[0mEpoch 0-234, Train loss: 2.37677, validation loss: 2.09149, learning rate: [0.025]\n","\u001b[32m[11/04 04:53:18 nl.defaults.trainer]: \u001b[0mEpoch 0-240, Train loss: 2.61317, validation loss: 2.36060, learning rate: [0.025]\n","\u001b[32m[11/04 04:53:24 nl.defaults.trainer]: \u001b[0mEpoch 0-247, Train loss: 2.30606, validation loss: 2.30186, learning rate: [0.025]\n","\u001b[32m[11/04 04:53:29 nl.defaults.trainer]: \u001b[0mEpoch 0-253, Train loss: 2.20843, validation loss: 2.28998, learning rate: [0.025]\n","\u001b[32m[11/04 04:53:34 nl.defaults.trainer]: \u001b[0mEpoch 0-259, Train loss: 2.09951, validation loss: 2.17767, learning rate: [0.025]\n","\u001b[32m[11/04 04:53:40 nl.defaults.trainer]: \u001b[0mEpoch 0-265, Train loss: 2.34517, validation loss: 2.41174, learning rate: [0.025]\n","\u001b[32m[11/04 04:53:45 nl.defaults.trainer]: \u001b[0mEpoch 0-271, Train loss: 2.69162, validation loss: 2.09757, learning rate: [0.025]\n","\u001b[32m[11/04 04:53:50 nl.defaults.trainer]: \u001b[0mEpoch 0-278, Train loss: 2.23705, validation loss: 2.20340, learning rate: [0.025]\n","\u001b[32m[11/04 04:53:56 nl.defaults.trainer]: \u001b[0mEpoch 0-284, Train loss: 2.19525, validation loss: 2.17231, learning rate: [0.025]\n","\u001b[32m[11/04 04:54:01 nl.defaults.trainer]: \u001b[0mEpoch 0-291, Train loss: 2.20197, validation loss: 2.13487, learning rate: [0.025]\n","\u001b[32m[11/04 04:54:07 nl.defaults.trainer]: \u001b[0mEpoch 0-297, Train loss: 2.22013, validation loss: 1.96754, learning rate: [0.025]\n","\u001b[32m[11/04 04:54:12 nl.defaults.trainer]: \u001b[0mEpoch 0-303, Train loss: 1.98788, validation loss: 2.86870, learning rate: [0.025]\n","\u001b[32m[11/04 04:54:17 nl.defaults.trainer]: \u001b[0mEpoch 0-309, Train loss: 2.21199, validation loss: 2.61859, learning rate: [0.025]\n","\u001b[32m[11/04 04:54:22 nl.defaults.trainer]: \u001b[0mEpoch 0-315, Train loss: 2.10242, validation loss: 2.25091, learning rate: [0.025]\n","\u001b[32m[11/04 04:54:27 nl.defaults.trainer]: \u001b[0mEpoch 0-321, Train loss: 2.10206, validation loss: 2.28463, learning rate: [0.025]\n","\u001b[32m[11/04 04:54:32 nl.defaults.trainer]: \u001b[0mEpoch 0-327, Train loss: 2.58212, validation loss: 2.39006, learning rate: [0.025]\n","\u001b[32m[11/04 04:54:37 nl.defaults.trainer]: \u001b[0mEpoch 0-333, Train loss: 1.99036, validation loss: 2.25023, learning rate: [0.025]\n","\u001b[32m[11/04 04:54:43 nl.defaults.trainer]: \u001b[0mEpoch 0-340, Train loss: 2.41273, validation loss: 2.10365, learning rate: [0.025]\n","\u001b[32m[11/04 04:54:48 nl.defaults.trainer]: \u001b[0mEpoch 0-346, Train loss: 2.47788, validation loss: 2.39633, learning rate: [0.025]\n","\u001b[32m[11/04 04:54:53 nl.defaults.trainer]: \u001b[0mEpoch 0-352, Train loss: 1.93434, validation loss: 2.19793, learning rate: [0.025]\n","\u001b[32m[11/04 04:54:58 nl.defaults.trainer]: \u001b[0mEpoch 0-358, Train loss: 2.14789, validation loss: 2.03500, learning rate: [0.025]\n","\u001b[32m[11/04 04:55:03 nl.defaults.trainer]: \u001b[0mEpoch 0-364, Train loss: 2.12296, validation loss: 1.97841, learning rate: [0.025]\n","\u001b[32m[11/04 04:55:08 nl.defaults.trainer]: \u001b[0mEpoch 0-370, Train loss: 1.97539, validation loss: 2.04377, learning rate: [0.025]\n","\u001b[32m[11/04 04:55:13 nl.defaults.trainer]: \u001b[0mEpoch 0-376, Train loss: 2.38178, validation loss: 1.92150, learning rate: [0.025]\n","\u001b[32m[11/04 04:55:19 nl.defaults.trainer]: \u001b[0mEpoch 0-382, Train loss: 2.14520, validation loss: 2.01233, learning rate: [0.025]\n","\u001b[32m[11/04 04:55:24 nl.defaults.trainer]: \u001b[0mEpoch 0-388, Train loss: 2.26313, validation loss: 2.07657, learning rate: [0.025]\n","\u001b[32m[11/04 04:55:25 nl.optimizers.oneshot.drnas.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n","tensor([-0.0192,  0.0253], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0122,  0.0171], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0163,  0.0264], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0194,  0.0237], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0049,  0.0127], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0092,  0.0327], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0025,  0.0074], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0177,  0.0213], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0240,  0.0320], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0251,  0.0265], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0191,  0.0255], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0065,  0.0071], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0056,  0.0131], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0250,  0.0306], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0276,  0.0272], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0183,  0.0213], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0180,  0.0193], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0283,  0.0403], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0274,  0.0316], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0294,  0.0404], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0250,  0.0369], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([ 0.0241,  0.0226, -0.0257], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([ 0.0096, -0.0026,  0.0090], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([ 0.0056, -0.0031,  0.0075], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([ 0.0059,  0.0071, -0.0057], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([ 0.0177,  0.0215, -0.0155], device='cuda:0', requires_grad=True)]\n","\u001b[32m[11/04 04:55:26 nl.defaults.trainer]: \u001b[0mEpoch 0 done. Train accuracy (top1, top5): 16.34400, 67.16400, Validation accuracy: 16.69198, 67.28341\n","\u001b[32m[11/04 04:55:26 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n","-0.019198, +0.025261, 1\n","-0.012175, +0.017090, 1\n","-0.016325, +0.026390, 1\n","-0.019404, +0.023698, 1\n","-0.004853, +0.012727, 1\n","-0.009179, +0.032665, 1\n","-0.002482, +0.007365, 1\n","-0.017747, +0.021259, 1\n","-0.023962, +0.032033, 1\n","-0.025142, +0.026530, 1\n","-0.019081, +0.025497, 1\n","-0.006501, +0.007114, 1\n","-0.005586, +0.013078, 1\n","-0.025004, +0.030632, 1\n","-0.027644, +0.027241, 1\n","-0.018299, +0.021272, 1\n","-0.018001, +0.019300, 1\n","-0.028257, +0.040262, 1\n","-0.027440, +0.031573, 1\n","-0.029357, +0.040434, 1\n","-0.024959, +0.036940, 1\n","+0.024094, +0.022552, -0.025677, 0\n","+0.009573, -0.002647, +0.009044, 0\n","+0.005567, -0.003084, +0.007470, 2\n","+0.005858, +0.007088, -0.005651, 1\n","+0.017665, +0.021489, -0.015534, 1\n","\u001b[32m[11/04 04:55:29 nl.defaults.trainer]: \u001b[0mEpoch 1-3, Train loss: 2.35668, validation loss: 2.31111, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:55:34 nl.defaults.trainer]: \u001b[0mEpoch 1-9, Train loss: 2.00450, validation loss: 2.17926, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:55:40 nl.defaults.trainer]: \u001b[0mEpoch 1-15, Train loss: 2.32270, validation loss: 1.87042, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:55:45 nl.defaults.trainer]: \u001b[0mEpoch 1-21, Train loss: 2.20105, validation loss: 2.03422, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:55:50 nl.defaults.trainer]: \u001b[0mEpoch 1-27, Train loss: 2.50805, validation loss: 2.03956, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:55:55 nl.defaults.trainer]: \u001b[0mEpoch 1-33, Train loss: 1.94862, validation loss: 2.08755, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:56:00 nl.defaults.trainer]: \u001b[0mEpoch 1-39, Train loss: 2.91165, validation loss: 1.83822, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:56:05 nl.defaults.trainer]: \u001b[0mEpoch 1-45, Train loss: 2.04095, validation loss: 2.23336, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:56:10 nl.defaults.trainer]: \u001b[0mEpoch 1-51, Train loss: 2.13359, validation loss: 2.21119, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:56:15 nl.defaults.trainer]: \u001b[0mEpoch 1-57, Train loss: 2.27135, validation loss: 2.22695, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:56:20 nl.defaults.trainer]: \u001b[0mEpoch 1-63, Train loss: 2.14415, validation loss: 2.09349, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:56:25 nl.defaults.trainer]: \u001b[0mEpoch 1-69, Train loss: 1.91145, validation loss: 2.07193, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:56:30 nl.defaults.trainer]: \u001b[0mEpoch 1-75, Train loss: 2.42895, validation loss: 1.96201, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:56:36 nl.defaults.trainer]: \u001b[0mEpoch 1-81, Train loss: 2.13029, validation loss: 2.88250, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:56:41 nl.defaults.trainer]: \u001b[0mEpoch 1-87, Train loss: 2.05118, validation loss: 1.99735, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:56:46 nl.defaults.trainer]: \u001b[0mEpoch 1-93, Train loss: 2.13345, validation loss: 2.15221, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:56:51 nl.defaults.trainer]: \u001b[0mEpoch 1-99, Train loss: 2.21276, validation loss: 2.20162, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:56:56 nl.defaults.trainer]: \u001b[0mEpoch 1-105, Train loss: 2.26367, validation loss: 2.27906, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:57:01 nl.defaults.trainer]: \u001b[0mEpoch 1-111, Train loss: 2.13817, validation loss: 2.01872, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:57:06 nl.defaults.trainer]: \u001b[0mEpoch 1-117, Train loss: 1.85159, validation loss: 1.82804, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:57:11 nl.defaults.trainer]: \u001b[0mEpoch 1-123, Train loss: 2.12599, validation loss: 2.27308, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:57:16 nl.defaults.trainer]: \u001b[0mEpoch 1-129, Train loss: 2.07691, validation loss: 1.89124, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:57:21 nl.defaults.trainer]: \u001b[0mEpoch 1-135, Train loss: 2.13821, validation loss: 2.50950, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:57:27 nl.defaults.trainer]: \u001b[0mEpoch 1-141, Train loss: 1.99409, validation loss: 2.17153, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:57:32 nl.defaults.trainer]: \u001b[0mEpoch 1-147, Train loss: 2.22296, validation loss: 1.96264, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:57:37 nl.defaults.trainer]: \u001b[0mEpoch 1-153, Train loss: 1.87365, validation loss: 1.94443, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:57:42 nl.defaults.trainer]: \u001b[0mEpoch 1-159, Train loss: 2.12520, validation loss: 1.92816, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:57:47 nl.defaults.trainer]: \u001b[0mEpoch 1-165, Train loss: 1.83028, validation loss: 2.15947, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:57:52 nl.defaults.trainer]: \u001b[0mEpoch 1-171, Train loss: 2.01636, validation loss: 1.82021, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:57:57 nl.defaults.trainer]: \u001b[0mEpoch 1-177, Train loss: 2.04845, validation loss: 2.06019, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:58:02 nl.defaults.trainer]: \u001b[0mEpoch 1-183, Train loss: 2.36359, validation loss: 2.38770, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:58:07 nl.defaults.trainer]: \u001b[0mEpoch 1-189, Train loss: 1.83809, validation loss: 1.81261, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:58:13 nl.defaults.trainer]: \u001b[0mEpoch 1-195, Train loss: 2.17513, validation loss: 2.04666, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:58:18 nl.defaults.trainer]: \u001b[0mEpoch 1-201, Train loss: 2.39943, validation loss: 2.70792, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:58:23 nl.defaults.trainer]: \u001b[0mEpoch 1-207, Train loss: 2.10951, validation loss: 1.93434, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:58:28 nl.defaults.trainer]: \u001b[0mEpoch 1-213, Train loss: 1.96199, validation loss: 1.83337, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:58:33 nl.defaults.trainer]: \u001b[0mEpoch 1-219, Train loss: 1.75809, validation loss: 2.04901, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:58:38 nl.defaults.trainer]: \u001b[0mEpoch 1-225, Train loss: 2.05996, validation loss: 2.25094, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:58:43 nl.defaults.trainer]: \u001b[0mEpoch 1-231, Train loss: 1.84483, validation loss: 2.10054, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:58:48 nl.defaults.trainer]: \u001b[0mEpoch 1-237, Train loss: 2.13639, validation loss: 2.16097, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:58:53 nl.defaults.trainer]: \u001b[0mEpoch 1-243, Train loss: 1.81921, validation loss: 1.74874, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:58:59 nl.defaults.trainer]: \u001b[0mEpoch 1-249, Train loss: 1.90421, validation loss: 2.25296, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:59:04 nl.defaults.trainer]: \u001b[0mEpoch 1-255, Train loss: 2.03373, validation loss: 2.03641, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:59:09 nl.defaults.trainer]: \u001b[0mEpoch 1-261, Train loss: 1.87973, validation loss: 2.06360, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:59:14 nl.defaults.trainer]: \u001b[0mEpoch 1-267, Train loss: 1.99685, validation loss: 2.07379, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:59:19 nl.defaults.trainer]: \u001b[0mEpoch 1-273, Train loss: 1.95473, validation loss: 2.27229, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:59:24 nl.defaults.trainer]: \u001b[0mEpoch 1-279, Train loss: 1.90053, validation loss: 1.86970, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:59:29 nl.defaults.trainer]: \u001b[0mEpoch 1-285, Train loss: 2.44619, validation loss: 2.07805, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:59:34 nl.defaults.trainer]: \u001b[0mEpoch 1-291, Train loss: 2.21156, validation loss: 2.23832, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:59:40 nl.defaults.trainer]: \u001b[0mEpoch 1-297, Train loss: 1.84784, validation loss: 2.13911, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:59:45 nl.defaults.trainer]: \u001b[0mEpoch 1-303, Train loss: 1.98936, validation loss: 2.02333, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:59:50 nl.defaults.trainer]: \u001b[0mEpoch 1-309, Train loss: 2.23484, validation loss: 1.89136, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 04:59:55 nl.defaults.trainer]: \u001b[0mEpoch 1-315, Train loss: 1.73067, validation loss: 1.98856, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 05:00:00 nl.defaults.trainer]: \u001b[0mEpoch 1-321, Train loss: 2.45119, validation loss: 1.77282, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 05:00:05 nl.defaults.trainer]: \u001b[0mEpoch 1-327, Train loss: 2.36704, validation loss: 1.92953, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 05:00:10 nl.defaults.trainer]: \u001b[0mEpoch 1-333, Train loss: 1.76042, validation loss: 1.97133, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 05:00:15 nl.defaults.trainer]: \u001b[0mEpoch 1-339, Train loss: 1.84148, validation loss: 2.03343, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 05:00:20 nl.defaults.trainer]: \u001b[0mEpoch 1-345, Train loss: 1.82402, validation loss: 1.95904, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 05:00:25 nl.defaults.trainer]: \u001b[0mEpoch 1-351, Train loss: 2.01129, validation loss: 1.94042, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 05:00:30 nl.defaults.trainer]: \u001b[0mEpoch 1-357, Train loss: 2.37213, validation loss: 2.10073, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 05:00:36 nl.defaults.trainer]: \u001b[0mEpoch 1-363, Train loss: 1.81043, validation loss: 2.13815, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 05:00:41 nl.defaults.trainer]: \u001b[0mEpoch 1-369, Train loss: 1.83363, validation loss: 1.77957, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 05:00:46 nl.defaults.trainer]: \u001b[0mEpoch 1-375, Train loss: 1.88449, validation loss: 1.89063, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 05:00:51 nl.defaults.trainer]: \u001b[0mEpoch 1-381, Train loss: 2.12381, validation loss: 2.21598, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 05:00:56 nl.defaults.trainer]: \u001b[0mEpoch 1-387, Train loss: 1.75400, validation loss: 1.89805, learning rate: [0.024412678195541847]\n","\u001b[32m[11/04 05:00:58 nl.optimizers.oneshot.drnas.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n","tensor([-0.0365,  0.0493], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0150,  0.0358], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0221,  0.0396], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0260,  0.0335], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([0.0045, 0.0099], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([0.0265, 0.0331], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0130,  0.0283], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0238,  0.0341], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0392,  0.0515], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0353,  0.0420], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0338,  0.0551], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0307,  0.0344], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0238,  0.0365], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0440,  0.0548], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0522,  0.0666], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0237,  0.0326], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0311,  0.0371], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0461,  0.0709], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0509,  0.0555], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0471,  0.0759], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0330,  0.0598], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([ 0.0516,  0.0332, -0.0417], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([ 0.0255, -0.0003,  0.0173], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([ 0.0237, -0.0008,  0.0107], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([ 0.0201,  0.0107, -0.0107], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([ 0.0372,  0.0309, -0.0231], device='cuda:0', requires_grad=True)]\n","\u001b[32m[11/04 05:00:59 nl.defaults.trainer]: \u001b[0mEpoch 1 done. Train accuracy (top1, top5): 24.23600, 80.39600, Validation accuracy: 24.72426, 80.52669\n","\u001b[32m[11/04 05:00:59 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n","-0.036547, +0.049338, 1\n","-0.015049, +0.035840, 1\n","-0.022106, +0.039634, 1\n","-0.026039, +0.033490, 1\n","+0.004522, +0.009888, 1\n","+0.026453, +0.033079, 1\n","-0.012963, +0.028291, 1\n","-0.023774, +0.034098, 1\n","-0.039205, +0.051497, 1\n","-0.035315, +0.042012, 1\n","-0.033794, +0.055071, 1\n","-0.030681, +0.034361, 1\n","-0.023779, +0.036476, 1\n","-0.043973, +0.054777, 1\n","-0.052162, +0.066597, 1\n","-0.023662, +0.032554, 1\n","-0.031056, +0.037137, 1\n","-0.046065, +0.070882, 1\n","-0.050860, +0.055531, 1\n","-0.047089, +0.075910, 1\n","-0.032980, +0.059778, 1\n","+0.051643, +0.033165, -0.041696, 0\n","+0.025512, -0.000277, +0.017273, 0\n","+0.023663, -0.000753, +0.010689, 0\n","+0.020068, +0.010699, -0.010660, 0\n","+0.037215, +0.030906, -0.023141, 0\n","\u001b[32m[11/04 05:01:01 nl.defaults.trainer]: \u001b[0mEpoch 2-2, Train loss: 2.05469, validation loss: 1.96138, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:01:07 nl.defaults.trainer]: \u001b[0mEpoch 2-8, Train loss: 2.09208, validation loss: 1.98492, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:01:12 nl.defaults.trainer]: \u001b[0mEpoch 2-14, Train loss: 2.11357, validation loss: 1.90536, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:01:17 nl.defaults.trainer]: \u001b[0mEpoch 2-20, Train loss: 2.09437, validation loss: 1.78765, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:01:22 nl.defaults.trainer]: \u001b[0mEpoch 2-26, Train loss: 1.99558, validation loss: 2.07236, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:01:27 nl.defaults.trainer]: \u001b[0mEpoch 2-32, Train loss: 2.32235, validation loss: 1.89345, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:01:32 nl.defaults.trainer]: \u001b[0mEpoch 2-38, Train loss: 1.75171, validation loss: 1.65521, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:01:37 nl.defaults.trainer]: \u001b[0mEpoch 2-44, Train loss: 1.98185, validation loss: 1.83211, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:01:42 nl.defaults.trainer]: \u001b[0mEpoch 2-50, Train loss: 1.68699, validation loss: 1.95517, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:01:47 nl.defaults.trainer]: \u001b[0mEpoch 2-56, Train loss: 2.10929, validation loss: 1.93039, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:01:53 nl.defaults.trainer]: \u001b[0mEpoch 2-62, Train loss: 1.76072, validation loss: 1.99510, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:01:58 nl.defaults.trainer]: \u001b[0mEpoch 2-68, Train loss: 1.97265, validation loss: 2.00725, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:02:03 nl.defaults.trainer]: \u001b[0mEpoch 2-74, Train loss: 2.52576, validation loss: 1.91171, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:02:08 nl.defaults.trainer]: \u001b[0mEpoch 2-80, Train loss: 1.91416, validation loss: 1.71960, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:02:13 nl.defaults.trainer]: \u001b[0mEpoch 2-86, Train loss: 1.61919, validation loss: 1.86229, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:02:18 nl.defaults.trainer]: \u001b[0mEpoch 2-92, Train loss: 2.14684, validation loss: 2.01433, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:02:23 nl.defaults.trainer]: \u001b[0mEpoch 2-98, Train loss: 1.56668, validation loss: 2.00504, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:02:28 nl.defaults.trainer]: \u001b[0mEpoch 2-104, Train loss: 2.00501, validation loss: 1.90787, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:02:34 nl.defaults.trainer]: \u001b[0mEpoch 2-110, Train loss: 1.76165, validation loss: 1.98636, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:02:39 nl.defaults.trainer]: \u001b[0mEpoch 2-116, Train loss: 2.05775, validation loss: 1.93613, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:02:44 nl.defaults.trainer]: \u001b[0mEpoch 2-122, Train loss: 1.58695, validation loss: 2.08103, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:02:49 nl.defaults.trainer]: \u001b[0mEpoch 2-128, Train loss: 1.82752, validation loss: 1.95374, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:02:54 nl.defaults.trainer]: \u001b[0mEpoch 2-134, Train loss: 2.10252, validation loss: 1.81954, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:02:59 nl.defaults.trainer]: \u001b[0mEpoch 2-140, Train loss: 1.91397, validation loss: 1.66525, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:03:04 nl.defaults.trainer]: \u001b[0mEpoch 2-146, Train loss: 1.88209, validation loss: 1.58720, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:03:09 nl.defaults.trainer]: \u001b[0mEpoch 2-152, Train loss: 1.86211, validation loss: 2.16949, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:03:14 nl.defaults.trainer]: \u001b[0mEpoch 2-158, Train loss: 1.76204, validation loss: 1.83746, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:03:20 nl.defaults.trainer]: \u001b[0mEpoch 2-164, Train loss: 1.91523, validation loss: 1.94327, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:03:25 nl.defaults.trainer]: \u001b[0mEpoch 2-170, Train loss: 1.87191, validation loss: 1.62612, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:03:30 nl.defaults.trainer]: \u001b[0mEpoch 2-176, Train loss: 2.00061, validation loss: 1.96814, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:03:35 nl.defaults.trainer]: \u001b[0mEpoch 2-182, Train loss: 1.86450, validation loss: 1.94699, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:03:40 nl.defaults.trainer]: \u001b[0mEpoch 2-188, Train loss: 1.89368, validation loss: 1.81218, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:03:45 nl.defaults.trainer]: \u001b[0mEpoch 2-194, Train loss: 1.54939, validation loss: 1.75328, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:03:50 nl.defaults.trainer]: \u001b[0mEpoch 2-200, Train loss: 2.03515, validation loss: 1.98373, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:03:55 nl.defaults.trainer]: \u001b[0mEpoch 2-206, Train loss: 1.90564, validation loss: 1.98884, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:04:00 nl.defaults.trainer]: \u001b[0mEpoch 2-212, Train loss: 2.00675, validation loss: 1.92336, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:04:06 nl.defaults.trainer]: \u001b[0mEpoch 2-218, Train loss: 1.81047, validation loss: 1.65093, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:04:11 nl.defaults.trainer]: \u001b[0mEpoch 2-224, Train loss: 2.05256, validation loss: 1.64419, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:04:16 nl.defaults.trainer]: \u001b[0mEpoch 2-230, Train loss: 1.91125, validation loss: 1.94692, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:04:21 nl.defaults.trainer]: \u001b[0mEpoch 2-236, Train loss: 1.77210, validation loss: 1.67097, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:04:26 nl.defaults.trainer]: \u001b[0mEpoch 2-242, Train loss: 1.89659, validation loss: 1.88025, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:04:31 nl.defaults.trainer]: \u001b[0mEpoch 2-248, Train loss: 2.06006, validation loss: 1.75520, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:04:36 nl.defaults.trainer]: \u001b[0mEpoch 2-254, Train loss: 1.70456, validation loss: 1.73647, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:04:41 nl.defaults.trainer]: \u001b[0mEpoch 2-260, Train loss: 1.69157, validation loss: 1.79295, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:04:46 nl.defaults.trainer]: \u001b[0mEpoch 2-266, Train loss: 1.78424, validation loss: 2.05852, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:04:52 nl.defaults.trainer]: \u001b[0mEpoch 2-272, Train loss: 1.71433, validation loss: 1.94457, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:04:57 nl.defaults.trainer]: \u001b[0mEpoch 2-278, Train loss: 1.59816, validation loss: 1.65445, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:05:02 nl.defaults.trainer]: \u001b[0mEpoch 2-284, Train loss: 1.84742, validation loss: 1.61241, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:05:07 nl.defaults.trainer]: \u001b[0mEpoch 2-290, Train loss: 1.84411, validation loss: 2.11370, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:05:12 nl.defaults.trainer]: \u001b[0mEpoch 2-296, Train loss: 1.55259, validation loss: 1.86160, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:05:17 nl.defaults.trainer]: \u001b[0mEpoch 2-302, Train loss: 1.93146, validation loss: 1.63712, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:05:22 nl.defaults.trainer]: \u001b[0mEpoch 2-308, Train loss: 1.64706, validation loss: 1.82189, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:05:28 nl.defaults.trainer]: \u001b[0mEpoch 2-314, Train loss: 1.74213, validation loss: 1.90140, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:05:33 nl.defaults.trainer]: \u001b[0mEpoch 2-320, Train loss: 1.55057, validation loss: 1.89830, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:05:38 nl.defaults.trainer]: \u001b[0mEpoch 2-326, Train loss: 1.78278, validation loss: 1.98695, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:05:43 nl.defaults.trainer]: \u001b[0mEpoch 2-332, Train loss: 1.71285, validation loss: 1.69596, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:05:48 nl.defaults.trainer]: \u001b[0mEpoch 2-338, Train loss: 1.95353, validation loss: 2.36207, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:05:53 nl.defaults.trainer]: \u001b[0mEpoch 2-344, Train loss: 1.88888, validation loss: 2.06558, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:05:58 nl.defaults.trainer]: \u001b[0mEpoch 2-350, Train loss: 2.15204, validation loss: 1.97216, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:06:03 nl.defaults.trainer]: \u001b[0mEpoch 2-356, Train loss: 1.77288, validation loss: 1.59370, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:06:08 nl.defaults.trainer]: \u001b[0mEpoch 2-362, Train loss: 1.72814, validation loss: 1.67818, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:06:14 nl.defaults.trainer]: \u001b[0mEpoch 2-368, Train loss: 1.95462, validation loss: 1.79661, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:06:19 nl.defaults.trainer]: \u001b[0mEpoch 2-374, Train loss: 1.98339, validation loss: 1.69242, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:06:24 nl.defaults.trainer]: \u001b[0mEpoch 2-380, Train loss: 2.06757, validation loss: 1.90654, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:06:29 nl.defaults.trainer]: \u001b[0mEpoch 2-386, Train loss: 1.84184, validation loss: 1.99359, learning rate: [0.022708203932499373]\n","\u001b[32m[11/04 05:06:32 nl.optimizers.oneshot.drnas.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n","tensor([-0.0553,  0.0824], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0221,  0.0626], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0293,  0.0599], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0293,  0.0409], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([0.0109, 0.0143], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([ 0.0945, -0.0005], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0199,  0.0452], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0360,  0.0486], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0496,  0.0637], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0422,  0.0532], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0261,  0.0640], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0515,  0.0583], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0401,  0.0602], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0703,  0.0808], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0738,  0.0992], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0354,  0.0494], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0479,  0.0585], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0561,  0.0950], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0662,  0.0725], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0478,  0.0939], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0421,  0.0858], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([ 0.0884,  0.0410, -0.0562], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([0.0553, 0.0019, 0.0085], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([ 0.0555,  0.0115, -0.0061], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([ 0.0517,  0.0154, -0.0266], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([ 0.0676,  0.0346, -0.0359], device='cuda:0', requires_grad=True)]\n","\u001b[32m[11/04 05:06:33 nl.defaults.trainer]: \u001b[0mEpoch 2 done. Train accuracy (top1, top5): 30.78800, 85.82800, Validation accuracy: 30.71451, 85.17024\n","\u001b[32m[11/04 05:06:33 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n","-0.055281, +0.082438, 1\n","-0.022076, +0.062596, 1\n","-0.029313, +0.059868, 1\n","-0.029339, +0.040925, 1\n","+0.010912, +0.014296, 1\n","+0.094452, -0.000505, 0\n","-0.019888, +0.045235, 1\n","-0.036036, +0.048609, 1\n","-0.049637, +0.063709, 1\n","-0.042164, +0.053210, 1\n","-0.026139, +0.064020, 1\n","-0.051470, +0.058317, 1\n","-0.040063, +0.060191, 1\n","-0.070290, +0.080757, 1\n","-0.073755, +0.099173, 1\n","-0.035416, +0.049383, 1\n","-0.047931, +0.058521, 1\n","-0.056082, +0.095047, 1\n","-0.066245, +0.072536, 1\n","-0.047795, +0.093910, 1\n","-0.042106, +0.085771, 1\n","+0.088350, +0.041014, -0.056220, 0\n","+0.055324, +0.001888, +0.008464, 0\n","+0.055546, +0.011489, -0.006062, 0\n","+0.051715, +0.015401, -0.026638, 0\n","+0.067610, +0.034616, -0.035928, 0\n","\u001b[32m[11/04 05:06:34 nl.defaults.trainer]: \u001b[0mEpoch 3-1, Train loss: 1.58404, validation loss: 2.12852, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:06:39 nl.defaults.trainer]: \u001b[0mEpoch 3-7, Train loss: 1.95853, validation loss: 1.53228, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:06:45 nl.defaults.trainer]: \u001b[0mEpoch 3-13, Train loss: 1.76211, validation loss: 1.75906, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:06:50 nl.defaults.trainer]: \u001b[0mEpoch 3-19, Train loss: 1.88198, validation loss: 1.59795, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:06:55 nl.defaults.trainer]: \u001b[0mEpoch 3-25, Train loss: 2.19271, validation loss: 1.90934, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:07:00 nl.defaults.trainer]: \u001b[0mEpoch 3-31, Train loss: 1.72351, validation loss: 1.58898, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:07:05 nl.defaults.trainer]: \u001b[0mEpoch 3-37, Train loss: 1.98018, validation loss: 2.06995, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:07:10 nl.defaults.trainer]: \u001b[0mEpoch 3-43, Train loss: 1.52005, validation loss: 1.64784, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:07:15 nl.defaults.trainer]: \u001b[0mEpoch 3-49, Train loss: 1.64654, validation loss: 1.85334, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:07:20 nl.defaults.trainer]: \u001b[0mEpoch 3-55, Train loss: 1.65304, validation loss: 1.97546, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:07:26 nl.defaults.trainer]: \u001b[0mEpoch 3-61, Train loss: 1.90784, validation loss: 1.53938, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:07:31 nl.defaults.trainer]: \u001b[0mEpoch 3-67, Train loss: 1.83520, validation loss: 1.49519, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:07:36 nl.defaults.trainer]: \u001b[0mEpoch 3-73, Train loss: 1.48269, validation loss: 1.73771, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:07:41 nl.defaults.trainer]: \u001b[0mEpoch 3-79, Train loss: 1.81285, validation loss: 1.69491, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:07:46 nl.defaults.trainer]: \u001b[0mEpoch 3-85, Train loss: 1.65613, validation loss: 1.78447, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:07:51 nl.defaults.trainer]: \u001b[0mEpoch 3-91, Train loss: 1.69086, validation loss: 1.96761, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:07:56 nl.defaults.trainer]: \u001b[0mEpoch 3-97, Train loss: 2.02978, validation loss: 1.57581, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:08:01 nl.defaults.trainer]: \u001b[0mEpoch 3-103, Train loss: 1.86399, validation loss: 1.74994, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:08:07 nl.defaults.trainer]: \u001b[0mEpoch 3-109, Train loss: 1.86580, validation loss: 1.71586, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:08:12 nl.defaults.trainer]: \u001b[0mEpoch 3-115, Train loss: 1.80600, validation loss: 1.60655, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:08:17 nl.defaults.trainer]: \u001b[0mEpoch 3-121, Train loss: 1.74653, validation loss: 1.63697, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:08:22 nl.defaults.trainer]: \u001b[0mEpoch 3-127, Train loss: 1.67485, validation loss: 1.73714, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:08:27 nl.defaults.trainer]: \u001b[0mEpoch 3-133, Train loss: 1.56734, validation loss: 1.81726, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:08:32 nl.defaults.trainer]: \u001b[0mEpoch 3-139, Train loss: 2.02128, validation loss: 1.76349, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:08:37 nl.defaults.trainer]: \u001b[0mEpoch 3-145, Train loss: 1.56709, validation loss: 1.76388, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:08:42 nl.defaults.trainer]: \u001b[0mEpoch 3-151, Train loss: 1.62322, validation loss: 1.98245, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:08:47 nl.defaults.trainer]: \u001b[0mEpoch 3-157, Train loss: 1.82747, validation loss: 1.43867, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:08:53 nl.defaults.trainer]: \u001b[0mEpoch 3-163, Train loss: 2.03728, validation loss: 1.49891, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:08:58 nl.defaults.trainer]: \u001b[0mEpoch 3-169, Train loss: 1.54990, validation loss: 1.91284, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:09:03 nl.defaults.trainer]: \u001b[0mEpoch 3-175, Train loss: 1.65041, validation loss: 1.96991, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:09:08 nl.defaults.trainer]: \u001b[0mEpoch 3-181, Train loss: 1.82475, validation loss: 1.75663, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:09:13 nl.defaults.trainer]: \u001b[0mEpoch 3-187, Train loss: 1.44088, validation loss: 1.82905, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:09:18 nl.defaults.trainer]: \u001b[0mEpoch 3-193, Train loss: 1.71795, validation loss: 1.59033, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:09:23 nl.defaults.trainer]: \u001b[0mEpoch 3-199, Train loss: 1.78326, validation loss: 1.91214, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:09:28 nl.defaults.trainer]: \u001b[0mEpoch 3-205, Train loss: 1.47858, validation loss: 1.64774, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:09:34 nl.defaults.trainer]: \u001b[0mEpoch 3-211, Train loss: 1.57550, validation loss: 1.73469, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:09:39 nl.defaults.trainer]: \u001b[0mEpoch 3-217, Train loss: 1.62871, validation loss: 1.64065, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:09:44 nl.defaults.trainer]: \u001b[0mEpoch 3-223, Train loss: 1.58323, validation loss: 1.49653, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:09:49 nl.defaults.trainer]: \u001b[0mEpoch 3-229, Train loss: 1.67354, validation loss: 1.79920, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:09:54 nl.defaults.trainer]: \u001b[0mEpoch 3-235, Train loss: 1.63991, validation loss: 1.70894, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:09:59 nl.defaults.trainer]: \u001b[0mEpoch 3-241, Train loss: 1.94539, validation loss: 1.99556, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:10:04 nl.defaults.trainer]: \u001b[0mEpoch 3-247, Train loss: 1.33198, validation loss: 1.85773, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:10:09 nl.defaults.trainer]: \u001b[0mEpoch 3-253, Train loss: 1.64670, validation loss: 1.82070, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:10:14 nl.defaults.trainer]: \u001b[0mEpoch 3-259, Train loss: 2.02961, validation loss: 1.67965, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:10:19 nl.defaults.trainer]: \u001b[0mEpoch 3-265, Train loss: 1.66352, validation loss: 1.47191, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:10:24 nl.defaults.trainer]: \u001b[0mEpoch 3-271, Train loss: 1.66940, validation loss: 1.60029, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:10:30 nl.defaults.trainer]: \u001b[0mEpoch 3-277, Train loss: 1.87833, validation loss: 1.67251, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:10:35 nl.defaults.trainer]: \u001b[0mEpoch 3-283, Train loss: 1.50362, validation loss: 2.06015, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:10:40 nl.defaults.trainer]: \u001b[0mEpoch 3-289, Train loss: 1.61353, validation loss: 1.82675, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:10:45 nl.defaults.trainer]: \u001b[0mEpoch 3-295, Train loss: 1.74142, validation loss: 1.69114, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:10:50 nl.defaults.trainer]: \u001b[0mEpoch 3-301, Train loss: 1.53560, validation loss: 1.84209, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:10:55 nl.defaults.trainer]: \u001b[0mEpoch 3-307, Train loss: 2.01874, validation loss: 1.78181, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:11:00 nl.defaults.trainer]: \u001b[0mEpoch 3-313, Train loss: 2.12934, validation loss: 1.57970, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:11:05 nl.defaults.trainer]: \u001b[0mEpoch 3-319, Train loss: 1.79573, validation loss: 1.63853, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:11:10 nl.defaults.trainer]: \u001b[0mEpoch 3-325, Train loss: 1.88177, validation loss: 1.44705, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:11:15 nl.defaults.trainer]: \u001b[0mEpoch 3-331, Train loss: 1.83799, validation loss: 1.57500, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:11:21 nl.defaults.trainer]: \u001b[0mEpoch 3-337, Train loss: 1.76123, validation loss: 1.58522, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:11:26 nl.defaults.trainer]: \u001b[0mEpoch 3-343, Train loss: 2.04242, validation loss: 1.72104, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:11:31 nl.defaults.trainer]: \u001b[0mEpoch 3-349, Train loss: 1.57264, validation loss: 1.60559, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:11:36 nl.defaults.trainer]: \u001b[0mEpoch 3-355, Train loss: 1.57363, validation loss: 1.60575, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:11:41 nl.defaults.trainer]: \u001b[0mEpoch 3-361, Train loss: 1.59794, validation loss: 1.87761, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:11:46 nl.defaults.trainer]: \u001b[0mEpoch 3-367, Train loss: 1.36427, validation loss: 1.74384, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:11:51 nl.defaults.trainer]: \u001b[0mEpoch 3-373, Train loss: 1.62724, validation loss: 1.59586, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:11:56 nl.defaults.trainer]: \u001b[0mEpoch 3-379, Train loss: 1.59354, validation loss: 1.61803, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:12:01 nl.defaults.trainer]: \u001b[0mEpoch 3-385, Train loss: 2.08414, validation loss: 1.56728, learning rate: [0.020053423027509683]\n","\u001b[32m[11/04 05:12:06 nl.optimizers.oneshot.drnas.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n","tensor([-0.0709,  0.1113], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0116,  0.0686], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0404,  0.0802], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0241,  0.0459], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([0.0174, 0.0122], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([ 0.1580, -0.0435], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0365,  0.0731], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0509,  0.0688], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0587,  0.0776], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0552,  0.0684], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0218,  0.0785], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0661,  0.0775], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0639,  0.0867], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0904,  0.1044], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0776,  0.1215], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0488,  0.0675], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0710,  0.0834], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0674,  0.1223], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0815,  0.0891], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0575,  0.1169], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0546,  0.1151], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([ 0.1133,  0.0559, -0.0682], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([0.0831, 0.0029, 0.0057], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([ 0.0855,  0.0079, -0.0128], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([ 0.0903,  0.0098, -0.0387], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([ 0.0935,  0.0422, -0.0473], device='cuda:0', requires_grad=True)]\n","\u001b[32m[11/04 05:12:06 nl.defaults.trainer]: \u001b[0mEpoch 3 done. Train accuracy (top1, top5): 36.25200, 88.46000, Validation accuracy: 36.70476, 88.63891\n","\u001b[32m[11/04 05:12:06 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n","-0.070893, +0.111329, 1\n","-0.011601, +0.068557, 1\n","-0.040421, +0.080240, 1\n","-0.024091, +0.045892, 1\n","+0.017372, +0.012230, 0\n","+0.158029, -0.043537, 0\n","-0.036536, +0.073060, 1\n","-0.050858, +0.068778, 1\n","-0.058730, +0.077638, 1\n","-0.055239, +0.068351, 1\n","-0.021844, +0.078529, 1\n","-0.066089, +0.077514, 1\n","-0.063905, +0.086727, 1\n","-0.090389, +0.104363, 1\n","-0.077570, +0.121456, 1\n","-0.048799, +0.067478, 1\n","-0.070976, +0.083379, 1\n","-0.067441, +0.122256, 1\n","-0.081451, +0.089110, 1\n","-0.057512, +0.116907, 1\n","-0.054591, +0.115118, 1\n","+0.113269, +0.055939, -0.068235, 0\n","+0.083089, +0.002864, +0.005710, 0\n","+0.085491, +0.007862, -0.012783, 0\n","+0.090299, +0.009782, -0.038723, 0\n","+0.093536, +0.042193, -0.047260, 0\n","\u001b[32m[11/04 05:12:07 nl.defaults.trainer]: \u001b[0mEpoch 4-0, Train loss: 1.60566, validation loss: 1.66342, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:12:12 nl.defaults.trainer]: \u001b[0mEpoch 4-6, Train loss: 1.77876, validation loss: 1.55825, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:12:17 nl.defaults.trainer]: \u001b[0mEpoch 4-12, Train loss: 1.38139, validation loss: 1.66702, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:12:22 nl.defaults.trainer]: \u001b[0mEpoch 4-18, Train loss: 1.71984, validation loss: 1.79887, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:12:27 nl.defaults.trainer]: \u001b[0mEpoch 4-24, Train loss: 1.55947, validation loss: 1.39138, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:12:33 nl.defaults.trainer]: \u001b[0mEpoch 4-30, Train loss: 1.89807, validation loss: 1.59732, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:12:38 nl.defaults.trainer]: \u001b[0mEpoch 4-36, Train loss: 1.35372, validation loss: 1.50814, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:12:43 nl.defaults.trainer]: \u001b[0mEpoch 4-42, Train loss: 1.72445, validation loss: 1.73379, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:12:48 nl.defaults.trainer]: \u001b[0mEpoch 4-48, Train loss: 1.87108, validation loss: 1.78092, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:12:53 nl.defaults.trainer]: \u001b[0mEpoch 4-54, Train loss: 1.75925, validation loss: 1.70830, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:12:58 nl.defaults.trainer]: \u001b[0mEpoch 4-60, Train loss: 1.61402, validation loss: 1.71985, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:13:03 nl.defaults.trainer]: \u001b[0mEpoch 4-66, Train loss: 1.35394, validation loss: 1.79881, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:13:08 nl.defaults.trainer]: \u001b[0mEpoch 4-72, Train loss: 1.56669, validation loss: 1.50138, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:13:14 nl.defaults.trainer]: \u001b[0mEpoch 4-78, Train loss: 1.68037, validation loss: 1.51431, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:13:19 nl.defaults.trainer]: \u001b[0mEpoch 4-84, Train loss: 1.81386, validation loss: 1.65506, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:13:24 nl.defaults.trainer]: \u001b[0mEpoch 4-90, Train loss: 1.92362, validation loss: 1.68518, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:13:29 nl.defaults.trainer]: \u001b[0mEpoch 4-96, Train loss: 1.90763, validation loss: 1.58433, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:13:34 nl.defaults.trainer]: \u001b[0mEpoch 4-102, Train loss: 1.90019, validation loss: 1.65633, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:13:39 nl.defaults.trainer]: \u001b[0mEpoch 4-108, Train loss: 1.37772, validation loss: 1.53920, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:13:44 nl.defaults.trainer]: \u001b[0mEpoch 4-114, Train loss: 1.66513, validation loss: 1.57662, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:13:49 nl.defaults.trainer]: \u001b[0mEpoch 4-120, Train loss: 1.42445, validation loss: 1.45404, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:13:55 nl.defaults.trainer]: \u001b[0mEpoch 4-126, Train loss: 1.55831, validation loss: 1.68587, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:14:00 nl.defaults.trainer]: \u001b[0mEpoch 4-132, Train loss: 1.62941, validation loss: 1.49812, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:14:05 nl.defaults.trainer]: \u001b[0mEpoch 4-138, Train loss: 1.54481, validation loss: 1.55519, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:14:10 nl.defaults.trainer]: \u001b[0mEpoch 4-144, Train loss: 1.66428, validation loss: 1.63897, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:14:15 nl.defaults.trainer]: \u001b[0mEpoch 4-150, Train loss: 1.82971, validation loss: 1.34377, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:14:20 nl.defaults.trainer]: \u001b[0mEpoch 4-156, Train loss: 1.84655, validation loss: 1.86236, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:14:25 nl.defaults.trainer]: \u001b[0mEpoch 4-162, Train loss: 1.58201, validation loss: 1.67717, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:14:30 nl.defaults.trainer]: \u001b[0mEpoch 4-168, Train loss: 1.67663, validation loss: 1.67544, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:14:35 nl.defaults.trainer]: \u001b[0mEpoch 4-174, Train loss: 1.81141, validation loss: 2.04040, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:14:41 nl.defaults.trainer]: \u001b[0mEpoch 4-180, Train loss: 1.38296, validation loss: 1.38667, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:14:46 nl.defaults.trainer]: \u001b[0mEpoch 4-186, Train loss: 1.33523, validation loss: 1.45363, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:14:51 nl.defaults.trainer]: \u001b[0mEpoch 4-192, Train loss: 1.58452, validation loss: 2.10576, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:14:56 nl.defaults.trainer]: \u001b[0mEpoch 4-198, Train loss: 1.48474, validation loss: 1.95188, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:15:01 nl.defaults.trainer]: \u001b[0mEpoch 4-204, Train loss: 1.19395, validation loss: 1.61448, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:15:06 nl.defaults.trainer]: \u001b[0mEpoch 4-210, Train loss: 1.69715, validation loss: 1.96682, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:15:11 nl.defaults.trainer]: \u001b[0mEpoch 4-216, Train loss: 1.35204, validation loss: 1.63332, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:15:16 nl.defaults.trainer]: \u001b[0mEpoch 4-222, Train loss: 1.32984, validation loss: 1.49123, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:15:21 nl.defaults.trainer]: \u001b[0mEpoch 4-228, Train loss: 1.60421, validation loss: 1.61857, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:15:27 nl.defaults.trainer]: \u001b[0mEpoch 4-234, Train loss: 1.58809, validation loss: 1.43738, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:15:32 nl.defaults.trainer]: \u001b[0mEpoch 4-240, Train loss: 1.65226, validation loss: 1.42475, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:15:37 nl.defaults.trainer]: \u001b[0mEpoch 4-246, Train loss: 1.52526, validation loss: 1.76165, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:15:42 nl.defaults.trainer]: \u001b[0mEpoch 4-252, Train loss: 1.70956, validation loss: 1.80872, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:15:47 nl.defaults.trainer]: \u001b[0mEpoch 4-258, Train loss: 1.50519, validation loss: 1.77507, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:15:52 nl.defaults.trainer]: \u001b[0mEpoch 4-264, Train loss: 1.53045, validation loss: 1.26367, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:15:57 nl.defaults.trainer]: \u001b[0mEpoch 4-270, Train loss: 1.78696, validation loss: 1.38136, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:16:02 nl.defaults.trainer]: \u001b[0mEpoch 4-276, Train loss: 1.66167, validation loss: 2.16171, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:16:07 nl.defaults.trainer]: \u001b[0mEpoch 4-282, Train loss: 1.56318, validation loss: 1.35472, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:16:13 nl.defaults.trainer]: \u001b[0mEpoch 4-288, Train loss: 1.48802, validation loss: 1.59007, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:16:18 nl.defaults.trainer]: \u001b[0mEpoch 4-294, Train loss: 1.91222, validation loss: 1.65267, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:16:23 nl.defaults.trainer]: \u001b[0mEpoch 4-300, Train loss: 1.49904, validation loss: 1.84677, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:16:28 nl.defaults.trainer]: \u001b[0mEpoch 4-306, Train loss: 1.60189, validation loss: 1.50899, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:16:33 nl.defaults.trainer]: \u001b[0mEpoch 4-312, Train loss: 1.65639, validation loss: 1.55249, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:16:38 nl.defaults.trainer]: \u001b[0mEpoch 4-318, Train loss: 1.57911, validation loss: 1.46320, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:16:43 nl.defaults.trainer]: \u001b[0mEpoch 4-324, Train loss: 1.67506, validation loss: 1.45834, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:16:48 nl.defaults.trainer]: \u001b[0mEpoch 4-330, Train loss: 1.93768, validation loss: 1.90879, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:16:54 nl.defaults.trainer]: \u001b[0mEpoch 4-336, Train loss: 1.54808, validation loss: 1.57787, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:16:59 nl.defaults.trainer]: \u001b[0mEpoch 4-342, Train loss: 1.33162, validation loss: 1.52340, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:17:04 nl.defaults.trainer]: \u001b[0mEpoch 4-348, Train loss: 2.00289, validation loss: 1.54299, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:17:09 nl.defaults.trainer]: \u001b[0mEpoch 4-354, Train loss: 1.58178, validation loss: 1.45942, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:17:14 nl.defaults.trainer]: \u001b[0mEpoch 4-360, Train loss: 1.62968, validation loss: 1.60363, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:17:19 nl.defaults.trainer]: \u001b[0mEpoch 4-366, Train loss: 1.26546, validation loss: 1.52807, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:17:24 nl.defaults.trainer]: \u001b[0mEpoch 4-372, Train loss: 1.67855, validation loss: 1.64232, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:17:30 nl.defaults.trainer]: \u001b[0mEpoch 4-378, Train loss: 1.84653, validation loss: 1.38729, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:17:35 nl.defaults.trainer]: \u001b[0mEpoch 4-384, Train loss: 1.67116, validation loss: 1.51626, learning rate: [0.016708203932499374]\n","\u001b[32m[11/04 05:17:40 nl.optimizers.oneshot.drnas.optimizer]: \u001b[0mArch weights before discretization: [Parameter containing:\n","tensor([-0.0802,  0.1336], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0063,  0.0751], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0332,  0.0860], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0139,  0.0493], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([0.0312, 0.0044], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([ 0.2315, -0.0977], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0411,  0.0888], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0567,  0.0785], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0467,  0.0710], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0695,  0.0874], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0022,  0.0747], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0858,  0.0996], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0643,  0.0986], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0971,  0.1145], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0550,  0.1249], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0529,  0.0792], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0829,  0.1012], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0569,  0.1349], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.1054,  0.1168], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0613,  0.1385], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([-0.0538,  0.1335], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([ 0.1528,  0.0637, -0.0912], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([ 0.1240, -0.0108, -0.0025], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([ 0.1158,  0.0124, -0.0217], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([ 0.1200,  0.0182, -0.0492], device='cuda:0', requires_grad=True), Parameter containing:\n","tensor([ 0.1346,  0.0378, -0.0580], device='cuda:0', requires_grad=True)]\n","\u001b[32m[11/04 05:17:40 nl.defaults.trainer]: \u001b[0mEpoch 4 done. Train accuracy (top1, top5): 41.92800, 90.68800, Validation accuracy: 42.05962, 90.50512\n","\u001b[32m[11/04 05:17:40 nl.optimizers.oneshot.darts.optimizer]: \u001b[0mArch weights (alphas, last column argmax): \n","-0.080190, +0.133649, 1\n","-0.006330, +0.075118, 1\n","-0.033214, +0.085973, 1\n","-0.013907, +0.049301, 1\n","+0.031150, +0.004367, 0\n","+0.231529, -0.097680, 0\n","-0.041137, +0.088773, 1\n","-0.056684, +0.078489, 1\n","-0.046689, +0.070972, 1\n","-0.069506, +0.087417, 1\n","-0.002198, +0.074698, 1\n","-0.085812, +0.099616, 1\n","-0.064269, +0.098627, 1\n","-0.097071, +0.114490, 1\n","-0.055009, +0.124908, 1\n","-0.052933, +0.079247, 1\n","-0.082923, +0.101174, 1\n","-0.056899, +0.134876, 1\n","-0.105363, +0.116838, 1\n","-0.061349, +0.138516, 1\n","-0.053762, +0.133536, 1\n","+0.152779, +0.063674, -0.091199, 0\n","+0.123970, -0.010834, -0.002536, 0\n","+0.115816, +0.012398, -0.021700, 0\n","+0.119956, +0.018222, -0.049168, 0\n","+0.134646, +0.037767, -0.058006, 0\n","\u001b[32m[11/04 05:17:41 nl.defaults.trainer]: \u001b[0mEpoch 5-0, Train loss: 1.37713, validation loss: 1.24098, learning rate: [0.013000000000000005]\n","\u001b[32m[11/04 05:17:46 nl.defaults.trainer]: \u001b[0mEpoch 5-6, Train loss: 1.39122, validation loss: 1.62833, learning rate: [0.013000000000000005]\n","\u001b[32m[11/04 05:17:51 nl.defaults.trainer]: \u001b[0mEpoch 5-12, Train loss: 1.66048, validation loss: 1.27214, learning rate: [0.013000000000000005]\n","\u001b[32m[11/04 05:17:56 nl.defaults.trainer]: \u001b[0mEpoch 5-18, Train loss: 1.57984, validation loss: 1.46635, learning rate: [0.013000000000000005]\n","\u001b[32m[11/04 05:18:02 nl.defaults.trainer]: \u001b[0mEpoch 5-24, Train loss: 1.39239, validation loss: 1.62063, learning rate: [0.013000000000000005]\n","\u001b[32m[11/04 05:18:07 nl.defaults.trainer]: \u001b[0mEpoch 5-30, Train loss: 1.42112, validation loss: 1.73056, learning rate: [0.013000000000000005]\n","\u001b[32m[11/04 05:18:12 nl.defaults.trainer]: \u001b[0mEpoch 5-36, Train loss: 1.90320, validation loss: 1.15251, learning rate: [0.013000000000000005]\n","\u001b[32m[11/04 05:18:17 nl.defaults.trainer]: \u001b[0mEpoch 5-42, Train loss: 1.43585, validation loss: 1.39989, learning rate: [0.013000000000000005]\n","\u001b[32m[11/04 05:18:22 nl.defaults.trainer]: \u001b[0mEpoch 5-48, Train loss: 1.35044, validation loss: 1.38172, learning rate: [0.013000000000000005]\n","\u001b[32m[11/04 05:18:27 nl.defaults.trainer]: \u001b[0mEpoch 5-54, Train loss: 1.29692, validation loss: 1.74243, learning rate: [0.013000000000000005]\n","\u001b[32m[11/04 05:18:32 nl.defaults.trainer]: \u001b[0mEpoch 5-60, Train loss: 1.45903, validation loss: 1.29024, learning rate: [0.013000000000000005]\n","\u001b[32m[11/04 05:18:37 nl.defaults.trainer]: \u001b[0mEpoch 5-66, Train loss: 1.48809, validation loss: 1.52157, learning rate: [0.013000000000000005]\n","\u001b[32m[11/04 05:18:42 nl.defaults.trainer]: \u001b[0mEpoch 5-72, Train loss: 1.54628, validation loss: 1.34910, learning rate: [0.013000000000000005]\n","\u001b[32m[11/04 05:18:48 nl.defaults.trainer]: \u001b[0mEpoch 5-78, Train loss: 1.43241, validation loss: 1.21199, learning rate: [0.013000000000000005]\n","\u001b[32m[11/04 05:18:53 nl.defaults.trainer]: \u001b[0mEpoch 5-84, Train loss: 1.31428, validation loss: 1.84564, learning rate: [0.013000000000000005]\n","\u001b[32m[11/04 05:18:58 nl.defaults.trainer]: \u001b[0mEpoch 5-90, Train loss: 1.63204, validation loss: 1.32506, learning rate: [0.013000000000000005]\n","\u001b[32m[11/04 05:19:03 nl.defaults.trainer]: \u001b[0mEpoch 5-96, Train loss: 1.67208, validation loss: 1.47886, learning rate: [0.013000000000000005]\n"]}],"source":["optimizer = supported_optimizers[\"drnas\"]\n","optimizer.adapt_search_space(search_space)\n","\n","# Start the search and evaluation\n","trainer = Trainer(optimizer, config)\n","\n","if not config.eval_only:\n","    checkpoint = utils.get_last_checkpoint(config) if config.resume else \"\"\n","    trainer.search(resume_from=checkpoint)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"id":"2AFA2AECAC474863A2ED05B7B22DE48A","jupyter":{},"notebookId":"635ff9c1addc7648de76ef79","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":["checkpoint = utils.get_last_checkpoint(config, search=False) if config.resume else \"\"\n","trainer.evaluate(resume_from=checkpoint)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"19DC189C78B0428E83F6FDD027679158","jupyter":{},"notebookId":"635ff9c1addc7648de76ef79","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["## Predictors and  evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"id":"4EDFDCF9333D48E6BBFF8F95DE32A4F2","jupyter":{},"notebookId":"635ff9c1addc7648de76ef79","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":["# Load the predictor evaluator and the predictor (XGBoost in this case)\n","from naslib.defaults.predictor_evaluator import PredictorEvaluator\n","from naslib.predictors import XGBoost\n","\n","# read the new configuration file that has the parameters of the predictor model\n","# NOTE: it is important to set config_type=\"predictor\" here\n","config = utils.get_config_from_args(args=[\"--config-file\", \"naslib/benchmarks/predictor_config.yaml\"], \n","                                    config_type=\"predictor\")\n","utils.set_seed(config.seed)\n","utils.log_args(config)\n","\n","logger = setup_logger(config.save + \"/log.log\")\n","logger.setLevel(logging.INFO)\n","\n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"FCEAE9108BC64D5A8C0C6BE0D726C5D7","jupyter":{},"notebookId":"635ff9c1addc7648de76ef79","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["Download the NAS-Bench-201 data from https://drive.google.com/file/d/17EBlTidimMaGrb3fE0APbljJl-ocgfs4/view?usp=sharing and place it in NASLib/naslib/data/."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"id":"828F506A1FF8474D89E8D4957E2177A8","jupyter":{},"notebookId":"635ff9c1addc7648de76ef79","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":["# this will load the NAS-Bench-101 data (architectures and their accuracy, runtime, etc).\n","dataset_api = get_dataset_api(config.search_space, config.dataset)\n","#dataset_api = get_dataset_api(config.search_space, config.dataset)\n","\n","# adapt the search space to the optimizer type\n","optimizer.adapt_search_space(search_space, dataset_api=dataset_api)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"id":"CBD5335BCEF44BDB839A2E2E77F00824","jupyter":{},"notebookId":"635ff9c1addc7648de76ef79","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":["# Now instantiate the predictor (every predictor works with certain encoding types for the architecture)\n","predictor = XGBoost(encoding_type='adjacency_one_hot', hpo_wrapper=False)\n","# Instantiate the evaluator\n","predictor_evaluator = PredictorEvaluator(predictor, config=config)\n","# similarly to the conventional NAS search that we saw before, the predictor evaluator also adapts to \n","# the search space at hand\n","predictor_evaluator.adapt_search_space(search_space, load_labeled=False, \n","                                       dataset_api=dataset_api)\n","# No search in this case. We only train the predictor on the training data and evaluate it on the test data.\n","# Note that the training data here is the pair (arch, performance) and not (image, label).\n","predictor_evaluator.evaluate()"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"id":"DE066D11CC1A4333ACFBE188F9E44B51","jupyter":{},"notebookId":"635ff9c1addc7648de76ef79","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"757720C940FA4DC5B919404A2489448F","jupyter":{},"notebookId":"635ff9c1addc7648de76ef79","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["## Using the predictors as surrogate models in Bayesian Optimization"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"7C31E97C59D14DB98BCD8A4258F9F18A","jupyter":{},"notebookId":"635ff9c1addc7648de76ef79","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["This code snippet shows how to run BANANAS with some of the performance predictors as surrogate models. We will do 3 trials with 3 different seeds of BANANAS for 5 iterations. For this we need to firstly generate the configuration files. The bash commands below do this."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"id":"89FBFB1451044AB48D168996403EC23C","jupyter":{},"notebookId":"635ff9c1addc7648de76ef79","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":["%%bash\n","optimizer=bananas\n","predictors=(mlp lgb xgb rf bayes_lin_reg gp)\n","\n","start_seed=0\n","\n","# folders:\n","# this supposes your location is at NASLib/docs. Change the base_file location based on where you\n","# opened the notebook\n","base_file=../naslib\n","save_dir=bananas_run\n","out_dir=$save_dir\\_$start_seed\n","\n","# search space / data:\n","search_space=nasbench201\n","dataset=cifar10\n","search_epochs=5\n","\n","# trials / seeds:\n","trials=3\n","end_seed=$(($start_seed + $trials - 1))\n","\n","# create config files\n","for i in $(seq 0 $((${#predictors[@]}-1)) )\n","do\n","    predictor=${predictors[$i]}\n","    python $base_file/benchmarks/create_configs.py --predictor $predictor \\\n","    --epochs $search_epochs --start_seed $start_seed --trials $trials \\\n","    --out_dir $out_dir --dataset=$dataset --config_type nas_predictor \\\n","    --search_space $search_space --optimizer $optimizer\n","done"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"6BC9B029698146569E15F41B00CEC25E","jupyter":{},"notebookId":"635ff9c1addc7648de76ef79","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["Write a function that gets a configuration file and optimizer as input and runs them."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"id":"DFBB470924C84533A2B75ADC207306F3","jupyter":{},"notebookId":"635ff9c1addc7648de76ef79","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":["from naslib.optimizers import Bananas\n","\n","def run_optimizer(config_file=\"bananas_run_0/cifar10/configs/nas_predictors/config_bananas_gp_0.yaml\",\n","                  nas_optimizer=Bananas) -> None:\n","    # TODO: add all the utilities, such as config file reading, logging as before.\n","    # afterwards instantiate the search space, optimizer, trainer and run the search + evaluation\n","    pass\n","run_optimizer()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}
