{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__version__\n",
      "optimizers\n",
      "predictors\n",
      "search_spaces\n",
      "utils\n"
     ]
    }
   ],
   "source": [
    "from pkgutil import iter_modules\n",
    "import naslib\n",
    "\n",
    "def list_submodules(module):\n",
    "    for submodule in iter_modules(module.__path__):\n",
    "        print(submodule.name)\n",
    "\n",
    "list_submodules(naslib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "device: cpu\n",
      "device: cpu\n",
      "device: cpu\n",
      "device: cpu\n",
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "from naslib.defaults.trainer import Trainer\n",
    "\n",
    "from naslib.search_spaces import (\n",
    "    DartsSearchSpace,\n",
    "    SimpleCellSearchSpace,\n",
    "    NasBench101SearchSpace,\n",
    "    HierarchicalSearchSpace,\n",
    ")\n",
    "from naslib.search_spaces.nasbench101 import graph\n",
    "\n",
    "from naslib.utils import get_dataset_api, setup_logger, utils\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = utils.get_config_from_args(config_type=\"nas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from file... This may take a few minutes...\n",
      "Loaded dataset in 5 seconds\n"
     ]
    }
   ],
   "source": [
    "dataset_api = get_dataset_api(config.search_space, config.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Cell Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = \"input\"\n",
    "OUTPUT = \"output\"\n",
    "CONV3X3 = \"conv3x3-bn-relu\"\n",
    "CONV1X1 = \"conv1x1-bn-relu\"\n",
    "MAXPOOL3X3 = \"maxpool3x3\"\n",
    "OPS = [CONV3X3, CONV1X1, MAXPOOL3X3]\n",
    "\n",
    "NUM_VERTICES = 7\n",
    "OP_SPOTS = NUM_VERTICES - 2\n",
    "MAX_EDGES = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_architecture(dataset_api, arch_limit=10):\n",
    "        \"\"\"\n",
    "        This will sample a random architecture and update the edges in the\n",
    "        naslib object accordingly.\n",
    "        From the NASBench repository:\n",
    "        one-hot adjacency matrix\n",
    "        draw [0,1] for each slot in the adjacency matrix\n",
    "        \"\"\"\n",
    "        architectures = []\n",
    "        while len(architectures) < arch_limit:\n",
    "            matrix = np.random.choice([0, 1], size=(NUM_VERTICES, NUM_VERTICES))\n",
    "            matrix = np.triu(matrix, 1)\n",
    "            ops = np.random.choice(OPS, size=NUM_VERTICES).tolist()\n",
    "            ops[0] = INPUT\n",
    "            ops[-1] = OUTPUT\n",
    "            spec = dataset_api[\"api\"].ModelSpec(matrix=matrix, ops=ops)\n",
    "            if dataset_api[\"nb101_data\"].is_valid(spec):\n",
    "                architectures.append({\"matrix\": matrix, \"ops\": ops})\n",
    "                #break\n",
    "        \n",
    "        return architectures\n",
    "            \n",
    "        #self.set_spec({\"matrix\": matrix, \"ops\": ops})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling the architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_architectures = sample_random_architecture(dataset_api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the architecture into Pytorch Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from naslib.predictors.utils.models import nasbench1 as nas101_arch\n",
    "from naslib.predictors.utils.models import nasbench1_spec\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dset\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arch_to_torch(arch, num_classes):\n",
    "    \"\"\"\n",
    "    Converts given architecture to Pytorch Neural Network.\n",
    "\n",
    "    Args:\n",
    "        arch (dict): Dict of matrix and operations.\n",
    "        num_classes (int): Number of output classes.\n",
    "\n",
    "    Returns:\n",
    "        Pytorch neural network.\n",
    "    \"\"\"\n",
    "    spec = nasbench1_spec._ToModelSpec(arch[\"matrix\"], arch[\"ops\"])\n",
    "\n",
    "    network = nas101_arch.Network(\n",
    "        spec,\n",
    "        stem_out=128,\n",
    "        num_stacks=3,\n",
    "        num_mods=3,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes_dic = {\"cifar10\": 10, \"cifar100\": 100, \"ImageNet16-120\": 120}\n",
    "num_class = num_classes_dic[\"cifar10\"]\n",
    "\n",
    "torch_networks = []\n",
    "\n",
    "for arch in sampled_architectures:\n",
    "    torch_networks.append(arch_to_torch(arch, num_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(torch_networks[0], './network0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_1 = torch.load('./network0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if the exported and the imported architectures are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_equality(model_1, model_2):\n",
    "    for p1, p2 in zip(model_1.parameters(), model_2.parameters()):\n",
    "        if p1.data.ne(p2.data).sum() > 0:\n",
    "            return False\n",
    "    \n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below loop checks if the models are equal. None of them is equal, so they are unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(torch_networks)): \n",
    "    for j in range(i+1, len(torch_networks)): \n",
    "        if check_model_equality(torch_networks[i], torch_networks[j]):\n",
    "            print(\"These models are equal;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the network for `CIFAR10` classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_project_root() -> Path:\n",
    "    \"\"\"\n",
    "    Returns the root path of the project.\n",
    "    \"\"\"\n",
    "    return Path('./').parent.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_device = torch_networks[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "naslib.predictors.utils.models.nasbench1.Network"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(on_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data transformations for `CIFAR10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cutout(object):\n",
    "    def __init__(self, length, prob=1.0):\n",
    "        self.length = length\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if np.random.binomial(1, self.prob):\n",
    "            h, w = img.size(1), img.size(2)\n",
    "            mask = np.ones((h, w), np.float32)\n",
    "            y = np.random.randint(h)\n",
    "            x = np.random.randint(w)\n",
    "\n",
    "            y1 = np.clip(y - self.length // 2, 0, h)\n",
    "            y2 = np.clip(y + self.length // 2, 0, h)\n",
    "            x1 = np.clip(x - self.length // 2, 0, w)\n",
    "            x2 = np.clip(x + self.length // 2, 0, w)\n",
    "\n",
    "            mask[y1:y2, x1:x2] = 0.0\n",
    "            mask = torch.from_numpy(mask)\n",
    "            mask = mask.expand_as(img)\n",
    "            img *= mask\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CIFAR_MEAN = [0.49139968, 0.48215827, 0.44653124]\n",
    "CIFAR_STD = [0.24703233, 0.24348505, 0.26158768]\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_transform.transforms.append(Cutout(16, 1))\n",
    "\n",
    "valid_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"{}/data\".format(get_project_root())\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data = dset.CIFAR10(\n",
    "    root=data, train=True, download=True, transform=train_transform\n",
    ")\n",
    "test_data = dset.CIFAR10(\n",
    "    root=data, train=False, download=True, transform=valid_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(0.8 * num_train))\n",
    "batch_size = 64\n",
    "\n",
    "train_queue = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[:split]),\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    "    worker_init_fn=np.random.seed(seed+1),\n",
    ")\n",
    "\n",
    "valid_queue = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[split:num_train]),\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    "    worker_init_fn=np.random.seed(seed),\n",
    ")\n",
    "\n",
    "test_queue = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    "    worker_init_fn=np.random.seed(seed),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining optimizer, lr, scheduler, loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = F.cross_entropy\n",
    "lr = 0.4\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=lr,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 625/625 [17:19<00:00,  1.66s/batch, accuracy=10.9, loss=nan]\n",
      "Epoch 2:  29%|██▉       | 184/625 [05:14<12:18,  1.68s/batch, accuracy=9.38, loss=nan]"
     ]
    }
   ],
   "source": [
    "network.train()\n",
    "for epoch in range(1, 5):\n",
    "    with tqdm(train_queue, unit=\"batch\") as tepoch:\n",
    "        for data, target in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            \n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = network(data)\n",
    "            predictions = output.argmax(dim=1, keepdim=True).squeeze()\n",
    "            loss = F.nll_loss(output, target)\n",
    "            correct = (predictions == target).sum().item()\n",
    "            accuracy = correct / batch_size\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tepoch.set_postfix(loss=loss.item(), accuracy=100. * accuracy)\n",
    "            sleep(0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('dlr-naslib')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9ddca5c0236aaa4ed72ead3a2ec19db198dc1705152d042dbce951924ecee52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
