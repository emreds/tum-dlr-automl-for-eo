{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'naslib.search_spaces.simple_cell'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15420/2805133426.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnaslib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/trial/lib/python3.7/site-packages/naslib/optimizers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moneshot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdarts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDARTSOptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moneshot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgsparsity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGSparseOptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moneshot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moneshot_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneShotNASOptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moneshot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrs_ws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomNASOptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moneshot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgdas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGDASOptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/trial/lib/python3.7/site-packages/naslib/optimizers/oneshot/darts/optimizer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnaslib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_spaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimitives\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMixedOp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnaslib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetaclasses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMetaOptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnaslib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcount_parameters_in_MB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/trial/lib/python3.7/site-packages/naslib/search_spaces/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msimple_cell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleCellSearchSpace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdarts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDartsSearchSpace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnasbench101\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNasBench101SearchSpace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnasbench201\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNasBench201SearchSpace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnasbenchnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNasBenchNLPSearchSpace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'naslib.search_spaces.simple_cell'"
     ]
    }
   ],
   "source": [
    "import naslib.optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'naslib.defaults'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15420/3251750941.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnaslib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m from naslib.search_spaces import (\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'naslib.defaults'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "from naslib.defaults.trainer import Trainer\n",
    "\n",
    "from naslib.search_spaces import (\n",
    "    DartsSearchSpace,\n",
    "    SimpleCellSearchSpace,\n",
    "    NasBench101SearchSpace,\n",
    "    HierarchicalSearchSpace,\n",
    ")\n",
    "from naslib.search_spaces.nasbench101 import graph\n",
    "\n",
    "from naslib.utils import get_dataset_api, setup_logger, utils\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = utils.get_config_from_args(config_type=\"nas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from file... This may take a few minutes...\n",
      "Loaded dataset in 6 seconds\n"
     ]
    }
   ],
   "source": [
    "dataset_api = get_dataset_api(config.search_space, config.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Cell Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = \"input\"\n",
    "OUTPUT = \"output\"\n",
    "CONV3X3 = \"conv3x3-bn-relu\"\n",
    "CONV1X1 = \"conv1x1-bn-relu\"\n",
    "MAXPOOL3X3 = \"maxpool3x3\"\n",
    "OPS = [CONV3X3, CONV1X1, MAXPOOL3X3]\n",
    "\n",
    "NUM_VERTICES = 7\n",
    "OP_SPOTS = NUM_VERTICES - 2\n",
    "MAX_EDGES = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_architecture(dataset_api, arch_limit=10):\n",
    "        \"\"\"\n",
    "        This will sample a random architecture and update the edges in the\n",
    "        naslib object accordingly.\n",
    "        From the NASBench repository:\n",
    "        one-hot adjacency matrix\n",
    "        draw [0,1] for each slot in the adjacency matrix\n",
    "        \"\"\"\n",
    "        architectures = []\n",
    "        while len(architectures) < arch_limit:\n",
    "            matrix = np.random.choice([0, 1], size=(NUM_VERTICES, NUM_VERTICES))\n",
    "            matrix = np.triu(matrix, 1)\n",
    "            ops = np.random.choice(OPS, size=NUM_VERTICES).tolist()\n",
    "            ops[0] = INPUT\n",
    "            ops[-1] = OUTPUT\n",
    "            spec = dataset_api[\"api\"].ModelSpec(matrix=matrix, ops=ops)\n",
    "            if dataset_api[\"nb101_data\"].is_valid(spec):\n",
    "                architectures.append({\"matrix\": matrix, \"ops\": ops})\n",
    "                #break\n",
    "        \n",
    "        return architectures\n",
    "            \n",
    "        #self.set_spec({\"matrix\": matrix, \"ops\": ops})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling the architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_architectures = sample_random_architecture(dataset_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'matrix': array([[0, 1, 0, 1, 1, 0, 1],\n",
       "         [0, 0, 1, 0, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0]]),\n",
       "  'ops': ['input',\n",
       "   'conv3x3-bn-relu',\n",
       "   'conv3x3-bn-relu',\n",
       "   'maxpool3x3',\n",
       "   'conv1x1-bn-relu',\n",
       "   'maxpool3x3',\n",
       "   'output']},\n",
       " {'matrix': array([[0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 1, 0, 1, 1],\n",
       "         [0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0]]),\n",
       "  'ops': ['input',\n",
       "   'conv1x1-bn-relu',\n",
       "   'maxpool3x3',\n",
       "   'maxpool3x3',\n",
       "   'maxpool3x3',\n",
       "   'conv1x1-bn-relu',\n",
       "   'output']},\n",
       " {'matrix': array([[0, 0, 1, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0]]),\n",
       "  'ops': ['input',\n",
       "   'maxpool3x3',\n",
       "   'maxpool3x3',\n",
       "   'maxpool3x3',\n",
       "   'conv1x1-bn-relu',\n",
       "   'conv3x3-bn-relu',\n",
       "   'output']},\n",
       " {'matrix': array([[0, 0, 0, 1, 1, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0, 1],\n",
       "         [0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0]]),\n",
       "  'ops': ['input',\n",
       "   'maxpool3x3',\n",
       "   'conv3x3-bn-relu',\n",
       "   'conv1x1-bn-relu',\n",
       "   'conv1x1-bn-relu',\n",
       "   'maxpool3x3',\n",
       "   'output']},\n",
       " {'matrix': array([[0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0]]),\n",
       "  'ops': ['input',\n",
       "   'conv1x1-bn-relu',\n",
       "   'conv3x3-bn-relu',\n",
       "   'conv3x3-bn-relu',\n",
       "   'conv1x1-bn-relu',\n",
       "   'conv3x3-bn-relu',\n",
       "   'output']},\n",
       " {'matrix': array([[0, 0, 1, 0, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0]]),\n",
       "  'ops': ['input',\n",
       "   'conv1x1-bn-relu',\n",
       "   'maxpool3x3',\n",
       "   'conv1x1-bn-relu',\n",
       "   'conv1x1-bn-relu',\n",
       "   'conv1x1-bn-relu',\n",
       "   'output']},\n",
       " {'matrix': array([[0, 1, 1, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0]]),\n",
       "  'ops': ['input',\n",
       "   'conv1x1-bn-relu',\n",
       "   'conv1x1-bn-relu',\n",
       "   'conv3x3-bn-relu',\n",
       "   'conv3x3-bn-relu',\n",
       "   'conv3x3-bn-relu',\n",
       "   'output']},\n",
       " {'matrix': array([[0, 0, 1, 1, 1, 0, 1],\n",
       "         [0, 0, 0, 0, 1, 0, 1],\n",
       "         [0, 0, 0, 1, 0, 1, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0]]),\n",
       "  'ops': ['input',\n",
       "   'conv3x3-bn-relu',\n",
       "   'maxpool3x3',\n",
       "   'conv1x1-bn-relu',\n",
       "   'maxpool3x3',\n",
       "   'conv1x1-bn-relu',\n",
       "   'output']},\n",
       " {'matrix': array([[0, 0, 1, 0, 1, 0, 1],\n",
       "         [0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 1, 1, 0, 1],\n",
       "         [0, 0, 0, 0, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0]]),\n",
       "  'ops': ['input',\n",
       "   'conv1x1-bn-relu',\n",
       "   'conv1x1-bn-relu',\n",
       "   'maxpool3x3',\n",
       "   'conv1x1-bn-relu',\n",
       "   'conv1x1-bn-relu',\n",
       "   'output']},\n",
       " {'matrix': array([[0, 0, 1, 0, 1, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0]]),\n",
       "  'ops': ['input',\n",
       "   'conv3x3-bn-relu',\n",
       "   'conv1x1-bn-relu',\n",
       "   'conv1x1-bn-relu',\n",
       "   'conv1x1-bn-relu',\n",
       "   'conv1x1-bn-relu',\n",
       "   'output']}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matrix': array([[0, 1, 0, 1, 1, 0, 1],\n",
       "        [0, 0, 1, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 0, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'ops': ['input',\n",
       "  'conv3x3-bn-relu',\n",
       "  'conv3x3-bn-relu',\n",
       "  'maxpool3x3',\n",
       "  'conv1x1-bn-relu',\n",
       "  'maxpool3x3',\n",
       "  'output']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_architectures[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the architecture into Pytorch Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from naslib.predictors.utils.models import nasbench1 as nas101_arch\n",
    "from naslib.predictors.utils.models import nasbench1_spec\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dset\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = nasbench1_spec._ToModelSpec(\n",
    "    sampled_architectures[0][\"matrix\"], sampled_architectures[0][\"ops\"]\n",
    ")\n",
    "\n",
    "num_classes_dic = {\"cifar10\": 10, \"cifar100\": 100, \"ImageNet16-120\": 120}\n",
    "\n",
    "network = nas101_arch.Network(\n",
    "    spec,\n",
    "    stem_out=128,\n",
    "    num_stacks=3,\n",
    "    num_mods=3,\n",
    "    num_classes=num_classes_dic[\"cifar10\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(network, './network0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_1 = torch.load('./network0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if the exported and the imported architectures are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_equality(model_1, model_2):\n",
    "    for p1, p2 in zip(model_1.parameters(), model_2.parameters()):\n",
    "        if p1.data.ne(p2.data).sum() > 0:\n",
    "            return False\n",
    "    \n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_model_equality(network, network_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the network for `CIFAR10` classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_project_root() -> Path:\n",
    "    \"\"\"\n",
    "    Returns the root path of the project.\n",
    "    \"\"\"\n",
    "    return Path('./').parent.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_device = network.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "naslib.predictors.utils.models.nasbench1.Network"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(on_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data transformations for `CIFAR10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cutout(object):\n",
    "    def __init__(self, length, prob=1.0):\n",
    "        self.length = length\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if np.random.binomial(1, self.prob):\n",
    "            h, w = img.size(1), img.size(2)\n",
    "            mask = np.ones((h, w), np.float32)\n",
    "            y = np.random.randint(h)\n",
    "            x = np.random.randint(w)\n",
    "\n",
    "            y1 = np.clip(y - self.length // 2, 0, h)\n",
    "            y2 = np.clip(y + self.length // 2, 0, h)\n",
    "            x1 = np.clip(x - self.length // 2, 0, w)\n",
    "            x2 = np.clip(x + self.length // 2, 0, w)\n",
    "\n",
    "            mask[y1:y2, x1:x2] = 0.0\n",
    "            mask = torch.from_numpy(mask)\n",
    "            mask = mask.expand_as(img)\n",
    "            img *= mask\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CIFAR_MEAN = [0.49139968, 0.48215827, 0.44653124]\n",
    "CIFAR_STD = [0.24703233, 0.24348505, 0.26158768]\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_transform.transforms.append(Cutout(16, 1))\n",
    "\n",
    "valid_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"{}/data\".format(get_project_root())\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data = dset.CIFAR10(\n",
    "    root=data, train=True, download=True, transform=train_transform\n",
    ")\n",
    "test_data = dset.CIFAR10(\n",
    "    root=data, train=False, download=True, transform=valid_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(0.8 * num_train))\n",
    "batch_size = 64\n",
    "\n",
    "train_queue = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[:split]),\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    "    worker_init_fn=np.random.seed(seed+1),\n",
    ")\n",
    "\n",
    "valid_queue = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[split:num_train]),\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    "    worker_init_fn=np.random.seed(seed),\n",
    ")\n",
    "\n",
    "test_queue = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    "    worker_init_fn=np.random.seed(seed),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining optimizer, lr, scheduler, loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = F.cross_entropy\n",
    "lr = 0.4\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=lr,\n",
    "                      momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 625/625 [17:19<00:00,  1.66s/batch, accuracy=10.9, loss=nan]\n",
      "Epoch 2:  29%|██▉       | 184/625 [05:14<12:18,  1.68s/batch, accuracy=9.38, loss=nan]"
     ]
    }
   ],
   "source": [
    "network.train()\n",
    "for epoch in range(1, 5):\n",
    "    with tqdm(train_queue, unit=\"batch\") as tepoch:\n",
    "        for data, target in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            \n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = network(data)\n",
    "            predictions = output.argmax(dim=1, keepdim=True).squeeze()\n",
    "            loss = F.nll_loss(output, target)\n",
    "            correct = (predictions == target).sum().item()\n",
    "            accuracy = correct / batch_size\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tepoch.set_postfix(loss=loss.item(), accuracy=100. * accuracy)\n",
    "            sleep(0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('trial')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e7b2cc1e0b55c2a6a7c213a39fd8cde303fbabb3cbcbdb3b3a2938b9e767cc2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
